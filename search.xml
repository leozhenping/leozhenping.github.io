<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker registry删除镜像方法]]></title>
    <url>%2F2018%2F09%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker-distribution主配置文件123456789101112131415161718192021config.yml: version: 0.1 log: fields: service: registry storage: delete: enabled: true #需要开启存储删除功能，才能使用接口删除镜像。 cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 删除Image的步骤 1、获取Etag字段中的sha256的字段 12345678910curl -i -X GET --header &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; http://localhost:5000/v2/镜像名称/manifests/标签 HTTP/1.1 200 OK Content-Length: 3047 Content-Type: application/vnd.docker.distribution.manifest.v2+json Docker-Content-Digest: sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 Docker-Distribution-Api-Version: registry/2.0 Etag: &quot;sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035&quot; #获取Etag字段的VaLue,sha256:xxx字段。 X-Content-Type-Options: nosniff Date: Wed, 11 Apr 2018 05:51:24 GMT 2、根据Etag字段来删除标签 123456789101112131415161718curl -k -v -s -X DELETE http://localhost:5000/v2/tomcat/manifests/ETAG * About to connect() to localhost port 5000 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 5000 (#0) &gt; DELETE /v2/tomcat/manifests/sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 HTTP/1.1 &gt; User-Agent: curl/7.29.0 &gt; Host: localhost:5000 &gt; Accept: */* &gt; &lt; HTTP/1.1 202 Accepted #返回202 Accepted内容，即表示删除成功 &lt; Docker-Distribution-Api-Version: registry/2.0 &lt; X-Content-Type-Options: nosniff &lt; Date: Wed, 11 Apr 2018 05:56:21 GMT &lt; Content-Length: 0 &lt; Content-Type: text/plain; charset=utf-8 &lt; * Connection #0 to host localhost left intact 3、获取标签，以确认删除标签成功 12curl -X GET http://localhost:5000/v2/tomcat/tags/list &#123;&quot;name&quot;:&quot;ubuntu&quot;,&quot;tags&quot;:null&#125; 4、释放磁盘空间 12345678910# registry garbage-collect /etc/docker-distribution/registry/config.yml 10 blobs marked, 15 blobs eligible for deletion #返回10个blobs被标记，15个适合删除的块。blob eligible for deletion: sha256:68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/68/68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830 go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09 #INFO信息中，返回的是: 块所对应的文件路径blob eligible for deletion:sha256:96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecdINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/96/96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecd go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09blob eligible for deletion: sha256:0b54f3727113712ee413d614a38f2a7e25c568d7c951c56487a90d4434426646## curl -X GET http://localhost:5000/v2/_catalog : 获取镜像的名称列表当删除镜像后，需要重新上传时，遇到错误: Layer already exists, 需要重启docker-distribution服务即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubernetes]]></title>
    <url>%2F2018%2F09%2F19%2Fkubernetes%2F</url>
    <content type="text"><![CDATA[安装harbor服务Harbor 安装前准备安装docker12345678# yum install -y yum-utils device-mapper-persistent-data lvm2# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum -y install docker-ce# docker --versionDocker version 17.06.2-ce, build cec0b72# systemctl start docker# systemctl status docker# systemctl enable docker 安装docker-compose1234# yum -y install python-pip# pip install --upgrade pip# pip install docker-compose# docker-compose version 安装 Habor12345678910111213141516171819# wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-offline-installer-v1.6.0.tgz# tar xf harbor-offline-installer-v1.6.0.tgz# cd harbor# docker load -i harbor.v1.6.0.tar.gz# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgoharbor/chartmuseum-photon v0.7.1-v1.6.0 99bfb7b3aa9c 4 weeks ago 357MBgoharbor/harbor-migrator v1.6.0 23ed5c5918a0 4 weeks ago 803MBgoharbor/redis-photon v1.6.0 745667dc5aa8 4 weeks ago 214MBgoharbor/clair-photon v2.0.5-v1.6.0 01cb5fff1728 4 weeks ago 308MBgoharbor/notary-server-photon v0.5.1-v1.6.0 11dfd338b15c 4 weeks ago 215MBgoharbor/notary-signer-photon v0.5.1-v1.6.0 08436cc747a3 4 weeks ago 212MBgoharbor/registry-photon v2.6.2-v1.6.0 1ec7d8d4f0fd 4 weeks ago 201MBgoharbor/nginx-photon v1.6.0 81df0f8a78c0 4 weeks ago 138MBgoharbor/harbor-log v1.6.0 0f474b9d4565 4 weeks ago 203MBgoharbor/harbor-jobservice v1.6.0 4e6a3afe6802 4 weeks ago 198MBgoharbor/harbor-ui v1.6.0 9cf3894e769e 4 weeks ago 221MBgoharbor/harbor-adminserver v1.6.0 14d9ee1bbda3 4 weeks ago 187MBgoharbor/harbor-db v1.6.0 5c39f18ce348 4 weeks ago 225MB 配置harbor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# sed -i &quot;s/reg.mydomain.com/registry01.isesol.local/g&quot; harbor.cfg# grep ^[a-z] harbor.cfg hostname = registry01.isesol.localui_url_protocol = httpmax_job_workers = 10 customize_crt = onssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.keysecretkey_path = /dataadmiral_url = NAlog_rotate_count = 50log_rotate_size = 200Mhttp_proxy =https_proxy =no_proxy = 127.0.0.1,localhost,ui,registryemail_identity = email_server = smtp.mydomain.comemail_server_port = 25email_username = admin@mydomain.comemail_password = abcemail_from = admin &lt;admin@mydomain.com&gt;email_ssl = falseemail_insecure = falseharbor_admin_password = 12345678auth_mode = ldap_authldap_url = ldap://172.20.10.61:389ldap_searchdn = uid=confluence,ou=users,dc=isesol,dc=localldap_search_pwd = 12345678ldap_basedn = ou=users,dc=isesol,dc=localldap_filter = (objectClass=person)ldap_uid = uidldap_scope = 3 ldap_timeout = 5ldap_verify_cert = trueldap_group_basedn = ou=users,dc=isesol,dc=localldap_group_filter = objectclass=groupldap_group_gid = cnldap_group_scope = 2self_registration = ontoken_expiration = 30project_creation_restriction = everyonedb_host = postgresqldb_password = root123db_port = 5432db_user = postgresredis_host = redisredis_port = 6379redis_password = redis_db_index = 1,2,3clair_db_host = postgresqlclair_db_password = root123clair_db_port = 5432clair_db_username = postgresclair_db = postgresclair_updaters_interval = 12uaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pemregistry_storage_provider_name = filesystemregistry_storage_provider_config =registry_custom_ca_bundle = 注:harbor 的主机名 hostname 不能注释再指定，必须删除默认设置再指定主机名，不然会产生错误。 安装 harbor1# ./install.sh 查看容器状况1234567891011# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------------------------harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpredis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp Harbor共由七个容器组成: harbor-adminserver:harbor系统管理服务 harbor-db: 由官方mysql镜像构成的数据库容器 harbor-jobservice:harbor的任务管理服务 harbor-log:harbor的日志收集、管理服务 harbor-ui:harbor的web页面服务 nginx:负责流量转发和安全验证 registry:官方的Docker registry，负责保存镜像 更改 docker 配置12345[root@registry01 harbor]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], #加速docker &quot;insecure-registries&quot;: [&quot;registry01.isesol.local&quot;] #加入非安全访问服务器&#125; 登陆 harbor1234567# docker login -u admin -p 12345678 registry01.isesol.localLogin Succeeded# docker login registry01.isesol.localUsername: adminPassword: Login Succeeded harbor服务管理12345678停止harbor# docker-compose stop启动harbor# docker-compose start重新Prepare# docker-compose down -v# ./prepare# docker-compose up -d harbor日志存放位置123456789# ll /var/log/harbor/-rw-r----- 1 10000 10000 602211 10月 10 11:08 adminserver.log-rw-r----- 1 10000 10000 46360 10月 9 18:06 jobservice.log-rw-r----- 1 10000 10000 25644 10月 9 17:07 mysql.log-rw-r----- 1 10000 10000 63488 10月 9 18:06 postgresql.log-rw-r----- 1 10000 10000 410516 10月 10 11:08 proxy.log-rw-r----- 1 10000 10000 160657 10月 10 11:05 redis.log-rw-r----- 1 10000 10000 319761 10月 10 11:08 registry.log-rw-r----- 1 10000 10000 1078369 10月 10 11:08 ui.log harbor ldap配置 Kubernetes安装环境说明123456789master01: 172.20.40.200master02: 172.20.40.201master03: 172.20.40.202node01: 172.20.40.203node02: 172.20.40.204node03: 172.20.40.205harbor: 172.20.40.199pod-network-cidr: 10.244.0.0/16 service-cidr: 10.96.0.0/12 非集群安装master配置yum仓库12345678910111213# cd /etc/yum.repos.d/# cat aliyun-k8s.repo [kubernetes-aliyun] name=aliyun kubernetes repo baseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot; gpgcheck=0 enabled=1# cat aliyun-docker.repo [aliyun-docker] name=aliyun docker baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/ enabled=1 gpgcheck=0 安装master节点关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# master01.isesol.local节点# yum install docker-ce kubelet kubeadm kubectl# systemctl start docker# systemctl enable kubelet# systemctl enable docker# 手动获取依赖镜像 #获取相应的images列表及版本 [root@master01 ~]# kubeadm config images list k8s.gcr.io/kube-apiserver:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.2.24 k8s.gcr.io/coredns:1.2.2 #从docker hub拉取镜像到本地 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-apiserver:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-controller-manager:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-scheduler:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@master01 ~]# docker pull mirrorgooglecontainers/etcd:3.2.24 [root@master01 ~]# docker pull coredns/coredns:1.2.2 # 重新打tag [root@master01 ~]# docker tag mirrorgooglecontainers/kube-apiserver:v1.12.1 k8s.gcr.io/kube-apiserver:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-scheduler:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@master01 ~]# docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24 [root@master01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2#初始化安装[root@master01 ~]# kubeadm init --kubernetes-version=v1.12.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 [init] using Kubernetes version: v1.12.1 [preflight] running pre-flight checks [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [preflight] Activating the kubelet service [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [172.20.40.200 127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [master01.isesol.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.20.40.200] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [certificates] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 25.003180 seconds [uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster [markmaster] Marking the node master01.isesol.local as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot; [markmaster] Marking the node master01.isesol.local as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master01.isesol.local&quot; as an annotation [bootstraptoken] using token: 4sjr1s.2agrbsj96c4hc17c [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc# 配置集群管理员 [root@master01 ~]# mkdir -p $HOME/.kube [root@master01 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master01 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看安装状态 [root@master01 manifests]# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-cncg6 0/1 Pending 0 5m13s coredns-576cbf47c7-cx9bg 0/1 Pending 0 5m13s etcd-master01.isesol.local 1/1 Running 0 4m25s kube-apiserver-master01.isesol.local 1/1 Running 0 4m20s kube-controller-manager-master01.isesol.local 1/1 Running 0 4m32s kube-proxy-n7bl5 1/1 Running 0 5m13s kube-scheduler-master01.isesol.local 1/1 Running 0 4m32s &gt; 此处coredns未ready, 查看报错信息为： 0/1 nodes are available: 1 node(s) had taints that the pod didn&apos;t tolerate，需要添加node节点 安装CNI12# docker pull quay.io/coreos/flannel:v0.10.0-amd64# apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装Node节点 在node01.isesol.local节点安装 关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务1234567891011[root@node01 ~]# yum install docker-ce kubelet kubeadm[root@node01 ~]# systemctl start docker[root@node01 ~]# systemctl enable docker kubelet# 准备镜像 [root@node01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@node01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@node01 ~]# docker pull coredns/coredns:1.2.2 [root@node01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@node01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@node01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2 节点加入集群123456789101112131415161718192021222324[root@node01 ~]# kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc [preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;] you can solve this problem with following methods: 1. Run &apos;modprobe -- &apos; to load missing kernel modules; 2. Provide the missing builtin kernel ipvs support [discovery] Trying to connect to API Server &quot;172.20.40.200:6443&quot; [discovery] Created cluster-info discovery client, requesting info from &quot;https://172.20.40.200:6443&quot; [discovery] Requesting info from &quot;https://172.20.40.200:6443&quot; again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.20.40.200:6443&quot; [discovery] Successfully established connection with API Server &quot;172.20.40.200:6443&quot; [kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [preflight] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;node01.isesol.local&quot; as an annotation This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 安装dashborad123456789101112131415161718192021# 在Node节点上准备镜像 # docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 # docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10# 在Master节点下安装Dashboard # wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml # vim kubernetes-dashboard.yaml kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort #添加此选项 配置dashboard的Token认证123456789101112131415161718192021222324252627282930313233# 创建dashboard-admin的ServiceAccountkubectl create serviceaccount dashboard-admin -n kube-system # 创建dashboard-cluster-admin的clusterrolebding，并绑定cluster-admin的clusterrole（集群管理员),关联kube-system空间下的dashboard-admin的Sakubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secret dashboad-admin-token-XXX -n kube-system #查询serviceaccount对应的secret # kubectl describe sa dashboard-admin -n kube-system Name: dashboard-admin Namespace: kube-system Labels: &lt;none&gt; Annotations: &lt;none&gt; Image pull secrets: &lt;none&gt; Mountable secrets: dashboard-admin-token-8zb56 Tokens: dashboard-admin-token-8zb56 #此tonken为sa绑定的secret Events: &lt;none&gt;# 获取secret的Token信息 # kubectl describe secret dashboard-admin-token-8zb56 -n kube-system Name: dashboard-admin-token-8zb56 Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 10a3b5d2-cc5d-11e8-ab5e-0050568faffd Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes #使用token登陆 token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tOHpiNTYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTBhM2I1ZDItY2M1ZC0xMWU4LWFiNWUtMDA1MDU2OGZhZmZkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.Pu8meO-UBK4F65sU2i56I01EnqP5Btzfl3eyNllb3kagLudEvFIkkpc5RpawsBlo4yzUFWT1HY50QlJKMbcJyHo7__O-sLkg88lLN_d9lX1usGYE2udMW112uLc52Y67e14Ni9KAju9RrvSCeCg-8PeR83k5f3Zq4ZetAXotSG0_NIkPQdgYP6jSmV3hugCHuk9-Cr0E5uSpr4-LcN81SEbLOwe0ar70K3JgkC7RwFpSRt-GuF8iUn-KZaBOqFAkIDSSc9aVhptgJ1b7dkSlIu_fT9yNq6p_BJ5RzwN3DJteY2POVitmKNivHCWD7K2ubo_XzimruIxVPj3qEkoLow 配置dashboard的kubeconfig认证kubeconfig支持两种类型绑定： Serviceaccount Useraccount 本节演示基于serviceaccount绑定认证，并使用Token认证中的Serviceaccount实现权限需要根据Serviceaccount的rolebinding实现此处只是将serviceaccount封装成了kubeconfig配置文件的形式 1234567# cd /etc/kubernetes/pki/# kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server=&quot;http://172.20.40.200:6443&quot; --embed-certs=true --kubeconfig=/root/cluster-admin.conf# kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; #需要找到serviceaccount对应的secret中的token字段# TOKENID=$(kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; | base64 -d) #使用base64解码并赋值给变量TOKENID# kubectl config set-credentials cluster-admin --token=$TOKENID --kubeconfig=/root/cluster-admin.conf# kubectl config set-context cluster-admin@kubernetes --cluster=kubernetes --user=cluster-admin --kubeconfig=/root/cluster-admin.conf# kubectl config use-context cluster-admin@kubernetes --kubeconfig=/root/cluster-admin.conf 生成kubeconfig文件的步骤 1234kubectl config set-cluster --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-credentials NAME --token=$KUBE_TOKEN --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-context kubectl config use-context]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
</search>
