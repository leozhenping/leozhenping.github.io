<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes高可用]]></title>
    <url>%2F2018%2F11%2F01%2Fkubernetes-ha%2F</url>
    <content type="text"><![CDATA[kubernetes高可用环境准备123456172.20.40.11 k8s-master01172.20.40.12 k8s-master02172.20.40.13 k8s-master03172.20.40.8 k8s-ha01172.20.40.9 k8s-ha02virtual ip: 172.20.40.10 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 配置haproxy和keepalivedhaproxy配置123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-ha01 ~]# cat /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth isesol:isesol stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:6443 mode tcp default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server k8s-master01 172.20.40.11:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master02 172.20.40.12:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master02 172.20.40.13:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 keepalived配置123456789101112131415161718192021222324252627282930[root@k8s-ha01 ~]# cat /etc/keepalived/keepalived.conf global_defs &#123; router_id 99&#125;vrrp_script chk_openvpn &#123; script &quot;/usr/bin/ps -ef | grep &apos;haproxy&apos; | grep -Ev &apos;(grep|monitor)&apos; | grep haproxy &amp;&gt; /dev/null&quot; interval 3 weight -20 user root&#125;vrrp_instance VI_1 &#123; state MASTER interface ens192 unicast_peer &#123; 172.20.40.9 &#125; virtual_router_id 52 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 9999999 &#125; virtual_ipaddress &#123; 172.20.40.10/24 dev ens192 label ens192:1 &#125; track_script &#123; chk_openvpn &#125;&#125; 安装docker配置仓库及安装123456[root@k8s-master01 yum.repos.d]# cat aliyun-docker.repo [aliyun-docker]name=aliyun dockerbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/enabled=1gpgcheck=0 由于官方kubernetes 1.11.X版本建议的docker版本为17.03.X，需要手动安装docker,也可以通过aliyun的仓库下载好相应的安装包，以本地安装的方式进行 1~]# yum localinstall docker-ce-17.03.3.ce-1.el7.x86_64.rpm docker-ce-selinux-17.03.3.ce-1.el7.noarch.rpm 配置服务12sudo systemctl start dockersudo systemctl enable docker.service 安装kubeadm、kubelet、kubectl服务配置仓库及安装123456[root@k8s-master01 yum.repos.d]# cat aliyun-k8s.repo [kubernetes-aliyun]name=aliyun kubernetes repobaseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot;gpgcheck=0enabled=1 123456789#安装指定版本yum install -y kubelet-1.11.4 kubeadm-1.11.4 kubectl-1.11.4 ipvsadm#加载ipvs模块modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4 配置kubelet12345678#获取docker的cgroup类型docker info | grep &apos;Cgroup&apos; | cut -d&apos; &apos; -f3#配置kubelet的cgroup类型# vi /etc/sysconfig/kubelet KUBELET_CGROUP_ARGS=&quot;--cgroup-driver=cgroupfs&quot;KUBELET_EXTRA_ARGS=&quot;--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1&quot;# systemctl daemon-reload 配置master节点 k8s-master01节点上操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 生成配置文件# CP0_IP=&quot;172.20.40.11&quot;# CP0_HOSTNAME=&quot;k8s-master01&quot;#cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.4imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- &quot;k8s-master01&quot;- &quot;k8s-master02&quot;- &quot;k8s-master03&quot;- &quot;172.20.40.11&quot;- &quot;172.20.40.12&quot;- &quot;172.20.40.13&quot;- &quot;172.20.40.10&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP0_IP controlPlaneEndpoint: 172.20.40.10:6443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP0_IP:2379&quot; advertise-client-urls: &quot;https://$CP0_IP:2379&quot; listen-peer-urls: &quot;https://$CP0_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP0_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380&quot; serverCertSANs: - $CP0_HOSTNAME - $CP0_IP peerCertSANs: - $CP0_HOSTNAME - $CP0_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: mode: ipvsEOF#拉取镜像# kubeadm config images pull --config kubeadm-master.config#初始化#kubeadm init --config kubeadm-master.config 输出：kubeadm join 172.20.40.10:6443 --token sjxhkf.h9jwg9r896hn7ua7 --discovery-token-ca-cert-hash sha256:b4969374793a7b55a9d71816a2d5c0ee466d68f0680f7edc8b01a3bb14286d0d 如果初始化过程出现问题，使用如下命令重置：kubeadm resetrm -rf /var/lib/cni/ $HOME/.kube/config 安装其他Master节点12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697传证书到master02#在master01节点打包证书cd /etc/kubernetes &amp;&amp; tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*scp k8s-key.tgz k8s-master02:~/scp k8s-key.tgz k8s-master03:~/CP0_IP=&quot;172.20.40.11&quot;CP0_HOSTNAME=&quot;k8s-master01&quot;CP1_IP=&quot;172.20.40.12&quot;CP1_HOSTNAME=&quot;k8s-master02&quot;cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- &quot;k8s-master01&quot;- &quot;k8s-master02&quot;- &quot;k8s-master03&quot;- &quot;172.20.40.11&quot;- &quot;172.20.40.12&quot;- &quot;172.20.40.13&quot;- &quot;172.20.40.10&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP1_IP controlPlaneEndpoint: 172.20.40.10:6443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP1_IP:2379&quot; advertise-client-urls: &quot;https://$CP1_IP:2379&quot; listen-peer-urls: &quot;https://$CP1_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP1_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380&quot; initial-cluster-state: existing serverCertSANs: - $CP1_HOSTNAME - $CP1_IP peerCertSANs: - $CP1_HOSTNAME - $CP1_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: mode: ipvsEOFmkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config#配置Kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubeconfig controller-manager --config kubeadm-master.configkubeadm alpha phase kubeconfig scheduler --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP=&quot;172.20.40.11&quot;CP0_HOSTNAME=&quot;k8s-master01&quot;CP1_IP=&quot;172.20.40.12&quot;CP1_HOSTNAME=&quot;k8s-master02&quot;kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行# kubeadm config images pull --config kubeadm-master.config# 部署# kubeadm alpha phase kubeconfig all --config kubeadm-master.config# kubeadm alpha phase controlplane all --config kubeadm-master.config# kubeadm alpha phase mark-master --config kubeadm-master.config k8s-master03上操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899传证书到master03#在master01节点打包证书cd /etc/kubernetes &amp;&amp; tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*scp k8s-key.tgz k8s-master02:~/scp k8s-key.tgz k8s-master03:~/CP0_IP=&quot;172.20.40.11&quot;CP0_HOSTNAME=&quot;k8s-master01&quot;CP1_IP=&quot;172.20.40.12&quot;CP1_HOSTNAME=&quot;k8s-master02&quot;CP2_IP=&quot;172.20.40.13&quot;CP2_HOSTNAME=&quot;k8s-master03&quot;cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- &quot;k8s-master01&quot;- &quot;k8s-master02&quot;- &quot;k8s-master03&quot;- &quot;172.20.40.11&quot;- &quot;172.20.40.12&quot;- &quot;172.20.40.13&quot;- &quot;172.20.40.10&quot;- &quot;127.0.0.1&quot;api: advertiseAddress: $CP2_IP controlPlaneEndpoint: 172.20.40.10:6443etcd: local: extraArgs: listen-client-urls: &quot;https://127.0.0.1:2379,https://$CP2_IP:2379&quot; advertise-client-urls: &quot;https://$CP2_IP:2379&quot; listen-peer-urls: &quot;https://$CP2_IP:2380&quot; initial-advertise-peer-urls: &quot;https://$CP2_IP:2380&quot; initial-cluster: &quot;$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380&quot; initial-cluster-state: existing serverCertSANs: - $CP2_HOSTNAME - $CP2_IP peerCertSANs: - $CP2_HOSTNAME - $CP2_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: mode: ipvsEOFmkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config#配置Kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubeconfig controller-manager --config kubeadm-master.configkubeadm alpha phase kubeconfig scheduler --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP=&quot;172.20.40.11&quot;CP0_HOSTNAME=&quot;k8s-master01&quot;CP2_IP=&quot;172.20.40.13&quot;CP2_HOSTNAME=&quot;k8s-master03&quot;kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行# kubeadm config images pull --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 安装flannel附件 在任意一台Master节点上操作 12# docker pull quay.io/coreos/flannel:v0.10.0-amd64# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 查看集群状态 在每台master节点上执行正常都应该返回如下结果 1234567891011121314151617181920212223242526[root@k8s-master02 manifests]# kubectl get epNAME ENDPOINTS AGEkubernetes 172.20.40.11:6443,172.20.40.12:6443,172.20.40.13:6443 16h[root@k8s-master01 ~]# kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-777d78ff6f-bhdfx 1/1 Running 0 13hcoredns-777d78ff6f-vhzp4 1/1 Running 0 13hetcd-k8s-master01 1/1 Running 2 13hetcd-k8s-master02 1/1 Running 0 12hetcd-k8s-master03 1/1 Running 0 23mkube-apiserver-k8s-master01 1/1 Running 0 13hkube-apiserver-k8s-master02 1/1 Running 0 12hkube-apiserver-k8s-master03 1/1 Running 0 23mkube-controller-manager-k8s-master01 1/1 Running 1 13hkube-controller-manager-k8s-master02 1/1 Running 0 12hkube-controller-manager-k8s-master03 1/1 Running 0 23mkube-flannel-ds-amd64-9gbv7 1/1 Running 0 24mkube-flannel-ds-amd64-bm97x 1/1 Running 0 12hkube-flannel-ds-amd64-glflv 1/1 Running 0 12hkube-proxy-27hwn 1/1 Running 0 13hkube-proxy-b4slt 1/1 Running 0 24mkube-proxy-r8b8s 1/1 Running 0 13hkube-scheduler-k8s-master01 1/1 Running 1 13hkube-scheduler-k8s-master02 1/1 Running 0 12hkube-scheduler-k8s-master03 1/1 Running 0 23m etcd服务管理查看集群健康状态12345[root@k8s-master01 ~]# etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/etcd/peer.crt --key-file=/etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.20.40.11:2379,https://172.20.40.12:2379 cluster-healthmember bc722a28e2be7cc is healthy: got healthy result from https://172.20.40.11:2379member 3e6c277b7e82f352 is healthy: got healthy result from https://172.20.40.12:2379member 904b39a41ef6abc8 is healthy: got healthy result from https://172.20.40.13:2379cluster is healthy 查看集群成员及状态1234[root@k8s-master01 ~]# etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/etcd/peer.crt --key-file=/etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.20.40.11:2379,https://172.20.40.12:2379 member listbc722a28e2be7cc: name=k8s-master01 peerURLs=https://172.20.40.11:2380 clientURLs=https://172.20.40.11:2379 isLeader=true #此节点是leader3e6c277b7e82f352: name=k8s-master02 peerURLs=https://172.20.40.12:2380 clientURLs=https://172.20.40.12:2379 isLeader=false904b39a41ef6abc8: name=k8s-master03 peerURLs=https://172.20.40.13:2380 clientURLs=https://172.20.40.13:2379 isLeader=false 管理节点12345678#删除故障节点## 获取节点信息[root@k8s-master01 ~]# etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/etcd/peer.crt --key-file=/etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.20.40.11:2379,https://172.20.40.12:2379 cluster-healthmember bc722a28e2be7cc is healthy: got healthy result from https://172.20.40.11:2379member 3e6c277b7e82f352 is healthy: got healthy result from https://172.20.40.12:2379member fe817f5c813e920c is unreachable: no available published client urls #此节点故障## 删除故障节点etcdctl --ca-file=/etc/kubernetes/pki/etcd/ca.crt --cert-file=/etc/kubernetes/pki/etcd/peer.crt --key-file=/etc/kubernetes/pki/etcd/peer.key --endpoints=https://172.20.40.11:2379,https://172.20.40.12:2379 member remove fe817f5c813e920c 访问etcd数据1234ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \--cert=/etc/kubernetes/pki/etcd/peer.crt \--key=/etc/kubernetes/pki/etcd/peer.key \get / --prefix --keys-only kubernetes的备份和恢复 借鉴文章，未按实验环境操作，文章出处:https://yq.aliyun.com/articles/336781 备份首先由于ETCD有三个备份，并且会同步，所以您只需要在一台master机器上执行ETCD备份即可。另外在运行下列命令前，确保当前机器的kube-apiserver是运行的。 12ps -ef|grep kube-apiserverroot 2063 2047 1 1月05 ? 00:41:01 kube-apiserver 执行备份命令 1234export ETCD_SERVERS=$(ps -ef|grep apiserver|grep -Eo &quot;etcd-servers=.*2379&quot;|awk -F= &apos;&#123;print $NF&#125;&apos;)mkdir -p /var/lib/etcd_backup/ETCDCTL_API=3 etcdctl snapshot --endpoints=$ETCD_SERVERS --cacert=/var/lib/etcd/cert/ca.pem --cert=/var/lib/etcd/cert/etcd-client.pem --key=/var/lib/etcd/cert/etcd-client-key.pem save /var/lib/etcd_backup/backup_$(date &quot;+%Y%m%d%H%M%S&quot;).dbSnapshot saved at /var/lib/etcd_backup/backup_20180107172459.db 执行完成后，您可以在/var/lib/etcd_backup中找到备份的snapshot12345[root@iZwz95q64qi83o88y9lq4cZ etcd_backup]# cd /var/lib/etcd_backup/[root@iZwz95q64qi83o88y9lq4cZ etcd_backup]# lsbackup_20180107172459.db[root@iZwz95q64qi83o88y9lq4cZ etcd_backup]# du -sh backup_20180107172459.db8.0M backup_20180107172459.db 恢复首先需要分别停掉三台Master机器的kube-apiserver 12mkdir -p /etc/kubernetes/manifests-backupsmv /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests-backups/ 确保kube-apiserver已经停止了,执行下列命令返回值为0 12ps -ef|grep kube-api|grep -v grep |wc -l0 分别在三台Master节点上，停止ETCD服务 1service etcd stop 确保ETCD停止成功12ps -ef|grep etcd|grep -v etcd|wc -l0 移除ETCD数据目录1mv /var/lib/etcd/data.etcd /var/lib/etcd/data.etcd_bak 分别在各个节点恢复数据,首先需要拷贝数据到每个master节点， 假设备份数据存在于/var/lib/etcd_backup/backup_20180107172459.db 123scp /var/lib/etcd_backup/backup_20180107172459.db root@master1:/var/lib/etcd_backup/scp /var/lib/etcd_backup/backup_20180107172459.db root@master2:/var/lib/etcd_backup/scp /var/lib/etcd_backup/backup_20180107172459.db root@master3:/var/lib/etcd_backup/ 执行恢复命令 123456789101112set -xexport ETCD_NAME=$(cat /usr/lib/systemd/system/etcd.service|grep ExecStart|grep -Eo &quot;name.*-name-[0-9].*--client&quot;|awk &apos;&#123;print $2&#125;&apos;)export ETCD_CLUSTER=$(cat /usr/lib/systemd/system/etcd.service|grep ExecStart|grep -Eo &quot;initial-cluster.*--initial&quot;|awk &apos;&#123;print $2&#125;&apos;)export ETCD_INITIAL_CLUSTER_TOKEN=$(cat /usr/lib/systemd/system/etcd.service|grep ExecStart|grep -Eo &quot;initial-cluster-token.*&quot;|awk &apos;&#123;print $2&#125;&apos;)export ETCD_INITIAL_ADVERTISE_PEER_URLS=$(cat /usr/lib/systemd/system/etcd.service|grep ExecStart|grep -Eo &quot;initial-advertise-peer-urls.*--listen-peer&quot;|awk &apos;&#123;print $2&#125;&apos;)ETCDCTL_API=3 etcdctl snapshot --cacert=/var/lib/etcd/cert/ca.pem --cert=/var/lib/etcd/cert/etcd-client.pem --key=/var/lib/etcd/cert/etcd-client-key.pem restore /var/lib/etcd_backup/backup_20180107172459.db \ --name $ETCD_NAME \ --data-dir /var/lib/etcd/data.etcd \ --initial-cluster $ETCD_CLUSTER \ --initial-cluster-token $ETCD_INITIAL_CLUSTER_TOKEN \ --initial-advertise-peer-urls $ETCD_INITIAL_ADVERTISE_PEER_URLSchown -R etcd:etcd /var/lib/etcd/data.etcd 分别在三个master节点启动ETCD，并且通过service命令确认启动成功 12# service etcd start# service etcd status 检查ETCD的健康 12345# export ETCD_SERVERS=$(cat /etc/kubernetes/manifests-backups/kube-apiserver.yaml |grep etcd-server|awk -F= &apos;&#123;print $2&#125;&apos;)ETCDCTL_API=3 etcdctl endpoint health --endpoints=$ETCD_SERVERS --cacert=/var/lib/etcd/cert/ca.pem --cert=/var/lib/etcd/cert/etcd-client.pem --key=/var/lib/etcd/cert/etcd-client-key.pemhttps://192.168.250.198:2379 is healthy: successfully committed proposal: took = 2.238886mshttps://192.168.250.196:2379 is healthy: successfully committed proposal: took = 3.390819mshttps://192.168.250.197:2379 is healthy: successfully committed proposal: took = 2.925103ms 如果ETCD是健康的，就到每台Master上恢复kube-apiserver 1# mv /etc/kubernetes/manifests-backups/kube-apiserver.yaml /etc/kubernetes/manifests/ 检查集群是否恢复正常，可以看到集群已经正常启动了。之前部署的应用也还在。12345678910111213141516# kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-2 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;etcd-1 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;# kubectl get noNAME STATUS ROLES AGE VERSIONcn-shenzhen.i-wz90xxpi51k2u51t5y0p Ready master 44d v1.8.4cn-shenzhen.i-wz93236e8pccdscwz3ha Ready master 44d v1.8.4cn-shenzhen.i-wz953xx6qnlzdi6vo2aa Ready &lt;none&gt; 44d v1.8.4cn-shenzhen.i-wz953xx6qnlzdi6vo2ab Ready &lt;none&gt; 44d v1.8.4kubectl get deployNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx 1 1 1 1 23d 总结： Kubernetes的备份主要是通过ETCD的备份完成的。而恢复时，主要考虑的是整个顺序：停止kube-apiserver，停止ETCD，恢复数据，启动ETCD，启动kube-apiserver Node节点加入集群1kubeadm join 172.20.40.10:6443 --token jzt3ut.56poosnt6x9gjvkn --discovery-token-ca-cert-hash sha256:4258cdfadd6aa65111cf28591eaf92aa106789e024b896f8f01617dfe66b1424 查看集群状态12345678910[root@k8s-master01 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master01 Ready master 17h v1.11.4k8s-master02 Ready master 17h v1.11.4k8s-master03 Ready master 4h v1.11.4k8s-node01 Ready &lt;none&gt; 7m v1.11.4k8s-node02 Ready &lt;none&gt; 7m v1.11.4k8s-node03 Ready &lt;none&gt; 7m v1.11.4k8s-node04 Ready &lt;none&gt; 7m v1.11.4k8s-node05 Ready &lt;none&gt; 7m v1.11.4 安装metrics-serve123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# https://github.com/kubernetes/kubernetes.git[root@k8s-master01 metrics-server]# cd cluster/addons/metrics-server/[root@k8s-master01 metrics-server]# ls auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml#修改配置下载下来的文件不能直接部署，有几处需要修改# cat resource-reader.yamlrules:- apiGroups: - &quot;&quot; resources: - pods - nodes - nodes/stats #新增这一行 - namespaces verbs: - get - list - watch # cat metrics-server-deployment.yaml #修改containers metrics-server 启动参数，修改好的如下： containers: - name: metrics-server image: k8s.gcr.io/metrics-server-amd64:v0.3.1 command: - /metrics-server - --metric-resolution=30s - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP # These are needed for GKE, which doesn&apos;t support secure communication yet. # Remove these lines for non-GKE clusters, and when GKE supports token-based auth. #- --kubelet-port=10255 #- --deprecated-kubelet-completely-insecure=true---- # 修改containers，metrics-server-nanny 启动参数，修改好的如下： command: - /pod_nanny - --config-dir=/etc/config - --cpu=80m - --extra-cpu=0.5m - --memory=80Mi - --extra-memory=8Mi - --threshold=5 - --deployment=metrics-server-v0.3.1 - --container=metrics-server - --poll-period=300000 - --estimator=exponential # Specifies the smallest cluster (defined in number of nodes) # resources will be scaled to. #- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125; #应用到集群 [root@k8s-master01 metrics-server]# kubectl create -f ./ clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created serviceaccount/metrics-server created configmap/metrics-server-config created deployment.extensions/metrics-server-v0.3.1 created service/metrics-server created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created #验证[root@k8s-master01 metrics-server]# kubectl get pod -n kube-system | grep metricmetrics-server-v0.3.1-57f4fdbd95-v89fd 2/2 Running 0 27m# 注意:在部署过程中遇到的错误：Get https://k8s-master:10250/stats/summary/: dial tcp: lookup k8s-master on 10.96.0.10:53: no such host提示 无法解析节点的主机名，是metrics-server这个容器不能通过CoreDNS 10.96.0.10:53 解析各Node的主机名，metrics-server连节点时默认是连接节点的主机名，需要加个参数，让它连接节点的IP：“--kubelet-preferred-address-types=InternalIP”因为10250是https端口，连接它时需要提供证书，所以加上--kubelet-insecure-tls，表示不验证客户端证书，此前的版本中使用--source=这个参数来指定不验证客户端证书。 IngressIngress暴露dashboard1234567891011121314151617181920212223242526openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj &quot;/CN=dashboard.isesoldev.com&quot;kubectl create secret tls dashboard-tls --key /tmp/tls.key --cert /tmp/tls.crt -n kube-systemapiVersion: extensions/v1beta1kind: Ingressmetadata: name: k8s-dashboard namespace: kube-system annotations: nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot; nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot; kubernetes.io/ingress.class: &quot;nginx&quot;spec: tls: - secretName: dashboard-tls rules: - host: dashboard.isesoldev.com http: paths: - path: backend: serviceName: kubernetes-dashboard servicePort: 443 Dashboarddashboard使用config文件登陆12345678910111213141516171819#创建sakubectl create serviceaccount dashboard-admin -n kube-system#获取secret~]# kubectl get secret -n kube-system | grep dashboard-admin dashboard-admin-token-l4h9d kubernetes.io/service-account-token 3 16m#将sa绑定到指定的权限kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin#获取sa的base64编码TOKENID=$(kubectl get secret dashboard-admin-token-l4h9d -n kube-system -o jsonpath=&#123;.data.token&#125; | base64 -d)#配置confiig文件cd /etc/kubernetes/pki/kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server=&quot;http://172.20.40.10:6443&quot; --embed-certs=true --kubeconfig=/root/dashboard-admin.confkubectl config set-credentials dashboard-admin --token=$TOKENID --kubeconfig=/root/dashboard-admin.confkubectl config set-context dashboard-admin@kubernetes --cluster=kubernetes --user=dashboard-admin --kubeconfig=/root/dashboard-admin.confkubectl config use-context dashboard-admin@kubernetes --kubeconfig=/root/dashboard-admin.confkubectl config view --kubeconfig=/root/dashboard-admin.conf]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker registry删除镜像方法]]></title>
    <url>%2F2018%2F09%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker-distribution主配置文件123456789101112131415161718192021config.yml: version: 0.1 log: fields: service: registry storage: delete: enabled: true #需要开启存储删除功能，才能使用接口删除镜像。 cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 删除Image的步骤 1、获取Etag字段中的sha256的字段 12345678910curl -i -X GET --header &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; http://localhost:5000/v2/镜像名称/manifests/标签 HTTP/1.1 200 OK Content-Length: 3047 Content-Type: application/vnd.docker.distribution.manifest.v2+json Docker-Content-Digest: sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 Docker-Distribution-Api-Version: registry/2.0 Etag: &quot;sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035&quot; #获取Etag字段的VaLue,sha256:xxx字段。 X-Content-Type-Options: nosniff Date: Wed, 11 Apr 2018 05:51:24 GMT 2、根据Etag字段来删除标签 123456789101112131415161718curl -k -v -s -X DELETE http://localhost:5000/v2/tomcat/manifests/ETAG * About to connect() to localhost port 5000 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 5000 (#0) &gt; DELETE /v2/tomcat/manifests/sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 HTTP/1.1 &gt; User-Agent: curl/7.29.0 &gt; Host: localhost:5000 &gt; Accept: */* &gt; &lt; HTTP/1.1 202 Accepted #返回202 Accepted内容，即表示删除成功 &lt; Docker-Distribution-Api-Version: registry/2.0 &lt; X-Content-Type-Options: nosniff &lt; Date: Wed, 11 Apr 2018 05:56:21 GMT &lt; Content-Length: 0 &lt; Content-Type: text/plain; charset=utf-8 &lt; * Connection #0 to host localhost left intact 3、获取标签，以确认删除标签成功 12curl -X GET http://localhost:5000/v2/tomcat/tags/list &#123;&quot;name&quot;:&quot;ubuntu&quot;,&quot;tags&quot;:null&#125; 4、释放磁盘空间 12345678910# registry garbage-collect /etc/docker-distribution/registry/config.yml 10 blobs marked, 15 blobs eligible for deletion #返回10个blobs被标记，15个适合删除的块。blob eligible for deletion: sha256:68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/68/68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830 go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09 #INFO信息中，返回的是: 块所对应的文件路径blob eligible for deletion:sha256:96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecdINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/96/96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecd go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09blob eligible for deletion: sha256:0b54f3727113712ee413d614a38f2a7e25c568d7c951c56487a90d4434426646## curl -X GET http://localhost:5000/v2/_catalog : 获取镜像的名称列表当删除镜像后，需要重新上传时，遇到错误: Layer already exists, 需要重启docker-distribution服务即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubernetes]]></title>
    <url>%2F2018%2F09%2F19%2Fkubernetes%2F</url>
    <content type="text"><![CDATA[安装harbor服务Harbor 安装前准备安装docker12345678# yum install -y yum-utils device-mapper-persistent-data lvm2# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum -y install docker-ce# docker --versionDocker version 17.06.2-ce, build cec0b72# systemctl start docker# systemctl status docker# systemctl enable docker 安装docker-compose1234# yum -y install python-pip# pip install --upgrade pip# pip install docker-compose# docker-compose version 安装 Habor12345678910111213141516171819# wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-offline-installer-v1.6.0.tgz# tar xf harbor-offline-installer-v1.6.0.tgz# cd harbor# docker load -i harbor.v1.6.0.tar.gz# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgoharbor/chartmuseum-photon v0.7.1-v1.6.0 99bfb7b3aa9c 4 weeks ago 357MBgoharbor/harbor-migrator v1.6.0 23ed5c5918a0 4 weeks ago 803MBgoharbor/redis-photon v1.6.0 745667dc5aa8 4 weeks ago 214MBgoharbor/clair-photon v2.0.5-v1.6.0 01cb5fff1728 4 weeks ago 308MBgoharbor/notary-server-photon v0.5.1-v1.6.0 11dfd338b15c 4 weeks ago 215MBgoharbor/notary-signer-photon v0.5.1-v1.6.0 08436cc747a3 4 weeks ago 212MBgoharbor/registry-photon v2.6.2-v1.6.0 1ec7d8d4f0fd 4 weeks ago 201MBgoharbor/nginx-photon v1.6.0 81df0f8a78c0 4 weeks ago 138MBgoharbor/harbor-log v1.6.0 0f474b9d4565 4 weeks ago 203MBgoharbor/harbor-jobservice v1.6.0 4e6a3afe6802 4 weeks ago 198MBgoharbor/harbor-ui v1.6.0 9cf3894e769e 4 weeks ago 221MBgoharbor/harbor-adminserver v1.6.0 14d9ee1bbda3 4 weeks ago 187MBgoharbor/harbor-db v1.6.0 5c39f18ce348 4 weeks ago 225MB 配置harbor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# sed -i &quot;s/reg.mydomain.com/registry01.isesol.local/g&quot; harbor.cfg# grep ^[a-z] harbor.cfg hostname = registry01.isesol.localui_url_protocol = httpmax_job_workers = 10 customize_crt = onssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.keysecretkey_path = /dataadmiral_url = NAlog_rotate_count = 50log_rotate_size = 200Mhttp_proxy =https_proxy =no_proxy = 127.0.0.1,localhost,ui,registryemail_identity = email_server = smtp.mydomain.comemail_server_port = 25email_username = admin@mydomain.comemail_password = abcemail_from = admin &lt;admin@mydomain.com&gt;email_ssl = falseemail_insecure = falseharbor_admin_password = 12345678auth_mode = ldap_authldap_url = ldap://172.20.10.61:389ldap_searchdn = uid=confluence,ou=users,dc=isesol,dc=localldap_search_pwd = 12345678ldap_basedn = ou=users,dc=isesol,dc=localldap_filter = (objectClass=person)ldap_uid = uidldap_scope = 3 ldap_timeout = 5ldap_verify_cert = trueldap_group_basedn = ou=users,dc=isesol,dc=localldap_group_filter = objectclass=groupldap_group_gid = cnldap_group_scope = 2self_registration = ontoken_expiration = 30project_creation_restriction = everyonedb_host = postgresqldb_password = root123db_port = 5432db_user = postgresredis_host = redisredis_port = 6379redis_password = redis_db_index = 1,2,3clair_db_host = postgresqlclair_db_password = root123clair_db_port = 5432clair_db_username = postgresclair_db = postgresclair_updaters_interval = 12uaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pemregistry_storage_provider_name = filesystemregistry_storage_provider_config =registry_custom_ca_bundle = 注:harbor 的主机名 hostname 不能注释再指定，必须删除默认设置再指定主机名，不然会产生错误。 安装 harbor1# ./install.sh 查看容器状况1234567891011# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------------------------harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpredis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp Harbor共由七个容器组成: harbor-adminserver:harbor系统管理服务 harbor-db: 由官方mysql镜像构成的数据库容器 harbor-jobservice:harbor的任务管理服务 harbor-log:harbor的日志收集、管理服务 harbor-ui:harbor的web页面服务 nginx:负责流量转发和安全验证 registry:官方的Docker registry，负责保存镜像 更改 docker 配置12345[root@registry01 harbor]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], #加速docker &quot;insecure-registries&quot;: [&quot;registry01.isesol.local&quot;] #加入非安全访问服务器&#125; 登陆 harbor1234567# docker login -u admin -p 12345678 registry01.isesol.localLogin Succeeded# docker login registry01.isesol.localUsername: adminPassword: Login Succeeded harbor服务管理12345678停止harbor# docker-compose stop启动harbor# docker-compose start重新Prepare# docker-compose down -v# ./prepare# docker-compose up -d harbor日志存放位置123456789# ll /var/log/harbor/-rw-r----- 1 10000 10000 602211 10月 10 11:08 adminserver.log-rw-r----- 1 10000 10000 46360 10月 9 18:06 jobservice.log-rw-r----- 1 10000 10000 25644 10月 9 17:07 mysql.log-rw-r----- 1 10000 10000 63488 10月 9 18:06 postgresql.log-rw-r----- 1 10000 10000 410516 10月 10 11:08 proxy.log-rw-r----- 1 10000 10000 160657 10月 10 11:05 redis.log-rw-r----- 1 10000 10000 319761 10月 10 11:08 registry.log-rw-r----- 1 10000 10000 1078369 10月 10 11:08 ui.log harbor ldap配置 Kubernetes安装环境说明123456789master01: 172.20.40.200master02: 172.20.40.201master03: 172.20.40.202node01: 172.20.40.203node02: 172.20.40.204node03: 172.20.40.205harbor: 172.20.40.199pod-network-cidr: 10.244.0.0/16 service-cidr: 10.96.0.0/12 非集群安装master配置yum仓库12345678910111213# cd /etc/yum.repos.d/# cat aliyun-k8s.repo [kubernetes-aliyun] name=aliyun kubernetes repo baseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot; gpgcheck=0 enabled=1# cat aliyun-docker.repo [aliyun-docker] name=aliyun docker baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/ enabled=1 gpgcheck=0 安装master节点关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# master01.isesol.local节点# yum install docker-ce kubelet kubeadm kubectl# systemctl start docker# systemctl enable kubelet# systemctl enable docker# 手动获取依赖镜像 #获取相应的images列表及版本 [root@master01 ~]# kubeadm config images list k8s.gcr.io/kube-apiserver:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.2.24 k8s.gcr.io/coredns:1.2.2 #从docker hub拉取镜像到本地 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-apiserver:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-controller-manager:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-scheduler:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@master01 ~]# docker pull mirrorgooglecontainers/etcd:3.2.24 [root@master01 ~]# docker pull coredns/coredns:1.2.2 # 重新打tag [root@master01 ~]# docker tag mirrorgooglecontainers/kube-apiserver:v1.12.1 k8s.gcr.io/kube-apiserver:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-scheduler:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@master01 ~]# docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24 [root@master01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2#初始化安装[root@master01 ~]# kubeadm init --kubernetes-version=v1.12.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 [init] using Kubernetes version: v1.12.1 [preflight] running pre-flight checks [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [preflight] Activating the kubelet service [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [172.20.40.200 127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [master01.isesol.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.20.40.200] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [certificates] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 25.003180 seconds [uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster [markmaster] Marking the node master01.isesol.local as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot; [markmaster] Marking the node master01.isesol.local as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master01.isesol.local&quot; as an annotation [bootstraptoken] using token: 4sjr1s.2agrbsj96c4hc17c [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc# 配置集群管理员 [root@master01 ~]# mkdir -p $HOME/.kube [root@master01 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master01 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看安装状态 [root@master01 manifests]# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-cncg6 0/1 Pending 0 5m13s coredns-576cbf47c7-cx9bg 0/1 Pending 0 5m13s etcd-master01.isesol.local 1/1 Running 0 4m25s kube-apiserver-master01.isesol.local 1/1 Running 0 4m20s kube-controller-manager-master01.isesol.local 1/1 Running 0 4m32s kube-proxy-n7bl5 1/1 Running 0 5m13s kube-scheduler-master01.isesol.local 1/1 Running 0 4m32s &gt; 此处coredns未ready, 查看报错信息为： 0/1 nodes are available: 1 node(s) had taints that the pod didn&apos;t tolerate，需要添加node节点 安装CNI12# docker pull quay.io/coreos/flannel:v0.10.0-amd64# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装Node节点 在node01.isesol.local节点安装 关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务1234567891011[root@node01 ~]# yum install docker-ce kubelet kubeadm[root@node01 ~]# systemctl start docker[root@node01 ~]# systemctl enable docker kubelet# 准备镜像 [root@node01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@node01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@node01 ~]# docker pull coredns/coredns:1.2.2 [root@node01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@node01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@node01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2 节点加入集群123456789101112131415161718192021222324[root@node01 ~]# kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc [preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;] you can solve this problem with following methods: 1. Run &apos;modprobe -- &apos; to load missing kernel modules; 2. Provide the missing builtin kernel ipvs support [discovery] Trying to connect to API Server &quot;172.20.40.200:6443&quot; [discovery] Created cluster-info discovery client, requesting info from &quot;https://172.20.40.200:6443&quot; [discovery] Requesting info from &quot;https://172.20.40.200:6443&quot; again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.20.40.200:6443&quot; [discovery] Successfully established connection with API Server &quot;172.20.40.200:6443&quot; [kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [preflight] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;node01.isesol.local&quot; as an annotation This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 安装dashborad123456789101112131415161718192021# 在Node节点上准备镜像 # docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 # docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10# 在Master节点下安装Dashboard # wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml # vim kubernetes-dashboard.yaml kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort #添加此选项 配置dashboard的Token认证123456789101112131415161718192021222324252627282930313233# 创建dashboard-admin的ServiceAccountkubectl create serviceaccount dashboard-admin -n kube-system # 创建dashboard-cluster-admin的clusterrolebding，并绑定cluster-admin的clusterrole（集群管理员),关联kube-system空间下的dashboard-admin的Sakubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secret dashboad-admin-token-XXX -n kube-system #查询serviceaccount对应的secret # kubectl describe sa dashboard-admin -n kube-system Name: dashboard-admin Namespace: kube-system Labels: &lt;none&gt; Annotations: &lt;none&gt; Image pull secrets: &lt;none&gt; Mountable secrets: dashboard-admin-token-8zb56 Tokens: dashboard-admin-token-8zb56 #此tonken为sa绑定的secret Events: &lt;none&gt;# 获取secret的Token信息 # kubectl describe secret dashboard-admin-token-8zb56 -n kube-system Name: dashboard-admin-token-8zb56 Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 10a3b5d2-cc5d-11e8-ab5e-0050568faffd Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes #使用token登陆 token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tOHpiNTYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTBhM2I1ZDItY2M1ZC0xMWU4LWFiNWUtMDA1MDU2OGZhZmZkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.Pu8meO-UBK4F65sU2i56I01EnqP5Btzfl3eyNllb3kagLudEvFIkkpc5RpawsBlo4yzUFWT1HY50QlJKMbcJyHo7__O-sLkg88lLN_d9lX1usGYE2udMW112uLc52Y67e14Ni9KAju9RrvSCeCg-8PeR83k5f3Zq4ZetAXotSG0_NIkPQdgYP6jSmV3hugCHuk9-Cr0E5uSpr4-LcN81SEbLOwe0ar70K3JgkC7RwFpSRt-GuF8iUn-KZaBOqFAkIDSSc9aVhptgJ1b7dkSlIu_fT9yNq6p_BJ5RzwN3DJteY2POVitmKNivHCWD7K2ubo_XzimruIxVPj3qEkoLow 配置dashboard的kubeconfig认证kubeconfig支持两种类型绑定： Serviceaccount Useraccount 本节演示基于serviceaccount绑定认证，并使用Token认证中的Serviceaccount实现权限需要根据Serviceaccount的rolebinding实现此处只是将serviceaccount封装成了kubeconfig配置文件的形式 1234567# cd /etc/kubernetes/pki/# kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server=&quot;http://172.20.40.200:6443&quot; --embed-certs=true --kubeconfig=/root/cluster-admin.conf# kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; #需要找到serviceaccount对应的secret中的token字段# TOKENID=$(kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; | base64 -d) #使用base64解码并赋值给变量TOKENID# kubectl config set-credentials cluster-admin --token=$TOKENID --kubeconfig=/root/cluster-admin.conf# kubectl config set-context cluster-admin@kubernetes --cluster=kubernetes --user=cluster-admin --kubeconfig=/root/cluster-admin.conf# kubectl config use-context cluster-admin@kubernetes --kubeconfig=/root/cluster-admin.conf 生成kubeconfig文件的步骤 1234kubectl config set-cluster --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-credentials NAME --token=$KUBE_TOKEN --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-context kubectl config use-context Glusterfs环境准备 环境说明 123456172.20.40.191 storage01.isesol.local storage01172.20.40.192 storage02.isesol.local storage02172.20.40.193 storage03.isesol.local storage03172.20.40.194 storage04.isesol.local storage04172.20.40.195 storage05.isesol.local storage05172.20.40.196 storage06.isesol.local storage06 配置时间同步 12~]# cat /etc/cron.d/timesync 00 */1 * * * /usr/sbin/ntpdate ntp.api.bz &amp;&gt; /dev/nul 配置各节点免密码登陆 各节点准备存储磁盘 12345~]# fdisk -l /dev/sdb 磁盘 /dev/sdb：536.9 GB, 536870912000 字节，1048576000 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节 关闭firewalld服务 12systemctl stop firewalld.servicesystemctl disable firewalld.service 存储节点加入集群配置yum仓库123456789101112131415161718192021222324252627282930[aliyun-docker]name=aliyun dockerbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/enabled=1gpgcheck=0[kubernetes-aliyun]name=aliyun kubernetes repobaseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot;gpgcheck=0enabled=1[base]name=aliyun base repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/os/x86_64/&quot;gpgcheck=0enabled=1[extras]name=aliyun extras repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/extras/x86_64/&quot;gpgcheck=0enabled=1[updates]name=aliyun updates repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/updates/x86_64/&quot;gpgcheck=0enabled=1[epel]name=apiyun epel repobaseurl=&quot;https://mirrors.aliyun.com/epel/7Server/x86_64/&quot;gpgcheck=0enabled=1 关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 安装服务123456789~]# yum install docker-ce kubelet kubeadm~]# systemctl start docker~]# systemctl enable docker kubelet # 准备镜像到本地 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 ~]# docker pull mirrorgooglecontainers/pause:3.1 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 节点加入集群1kubeadm join 172.20.40.200:6443 --token q1lu4f.8v3frs2mlvxmy0r5 --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc token TTL由于token的TTL默认是24个小时，如果超出这个时间，使用过期的token加入集群，会报如下错误：12345678910111213141516171819202122232425262728293031323334[root@storage01 kubernetes]# kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc [preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;]you can solve this problem with following methods: 1. Run &apos;modprobe -- &apos; to load missing kernel modules;2. Provide the missing builtin kernel ipvs support[discovery] Trying to connect to API Server &quot;172.20.40.200:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.20.40.200:6443&quot;[discovery] Requesting info from &quot;https://172.20.40.200:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.20.40.200:6443&quot;[discovery] Successfully established connection with API Server &quot;172.20.40.200:6443&quot;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespaceUnauthorized#查看token的状态 ~]# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS4sjr1s.2agrbsj96c4hc17c &lt;invalid&gt; 2018-10-11T13:01:14+08:00 authentication,signing The default bootstrap token generated by &apos;kubeadm init&apos;. system:bootstrappers:kubeadm:default-node-token# 需要手动创建一个新的Token~]# kubeadm token create~]# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSq1lu4f.8v3frs2mlvxmy0r5 23h 2018-10-17T12:11:04+08:00 authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token#获取ca证书sha256编码hash值# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &apos;s/^.* //&apos;0fd95a9bc67a7bf0ef42da968a0d55d92e52898ec37c971bd77ee501d845b538#使用新的token加入集群kubeadm join 172.20.40.200:6443 --token q1lu4f.8v3frs2mlvxmy0r5 --discovery-token-ca-cert-hash sha256:0fd95a9bc67a7bf0ef42da968a0d55d92e52898ec37c971bd77ee501d845b538 查看node节点状态1234567891011[root@master01 pki]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster01.isesol.local Ready master 5d23h v1.12.1node01.isesol.local Ready &lt;none&gt; 5d23h v1.12.1node02.isesol.local Ready &lt;none&gt; 5d22h v1.12.1storage01.isesol.local Ready &lt;none&gt; 10m v1.12.1storage02.isesol.local Ready &lt;none&gt; 9m27s v1.12.1storage03.isesol.local Ready &lt;none&gt; 9m24s v1.12.1storage04.isesol.local Ready &lt;none&gt; 9m20s v1.12.1storage05.isesol.local Ready &lt;none&gt; 9m17s v1.12.1storage06.isesol.local Ready &lt;none&gt; 8m18s v1.12.1 配置存储节点添加taints123456kubectl label node GFS-NODE storage=glusterfskubectl label node GFS-NODE storage=glusterfskubectl label node GFS-NODE storage=glusterfskubectl label node GFS-NODE storage=glusterfskubectl label node GFS-NODE storage=glusterfskubectl label node GFS-NODE storage=glusterfs 添加标签和taint12345678910111213141516kubectl taint node storage01.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage02.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage03.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage04.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage05.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage06.isesol.local node-role.kubernetes.io/storage=:NoSchedule#验证配置[root@master01 ~]# kubectl get node -l storage=glusterfsNAME STATUS ROLES AGE VERSIONstorage01.isesol.local Ready &lt;none&gt; 57m v1.12.1storage02.isesol.local Ready &lt;none&gt; 55m v1.12.1storage03.isesol.local Ready &lt;none&gt; 55m v1.12.1storage04.isesol.local Ready &lt;none&gt; 55m v1.12.1storage05.isesol.local Ready &lt;none&gt; 55m v1.12.1storage06.isesol.local Ready &lt;none&gt; 54m v1.12.1 glusterfs和Heketi 借鉴的技术文章：http://blog.51cto.com/newfly 下载1git clone https://github.com/heketi/heketi.git 配置存储节点12345modprobe rdma_cmmodprobe dm_thin_poolecho &quot;modprobe rdma_cm&quot; &gt;&gt; /etc/rc.localecho &quot;modprobe dm_thin_pool&quot; &gt;&gt; /etc/rc.localchmod +x /etc/rc.local 配置gluster daemonset12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455cd heketi/extras/kubernetes/vim glusterfs-daemonse.json&#123; &quot;kind&quot;: &quot;DaemonSet&quot;, &quot;apiVersion&quot;: &quot;apps/v1&quot;, &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;glusterfs&quot;, &quot;annotations&quot;: &#123; &quot;description&quot;: &quot;GlusterFS Daemon Set&quot;, &quot;tags&quot;: &quot;glusterfs&quot; &#125; &#125;, &quot;spec&quot;: &#123; &quot;selector&quot; : &#123; &quot;matchLabels&quot; : &#123; &quot;storage&quot;: &quot;glusterfs&quot;, &quot;glusterfs-node&quot;: &quot;daemonset&quot; &#125; &#125;, &quot;template&quot;: &#123; &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;glusterfs&quot;, &quot;labels&quot;: &#123; &quot;storage&quot;: &quot;glusterfs&quot; &#125; &#125;, &quot;spec&quot;: &#123; &quot;affinity&quot;: &#123; &quot;nodeAffinity&quot; : &#123; &quot;requiredDuringSchedulingIgnoredDuringExecution&quot; : &#123; &quot;nodeSelectorTerms&quot; : [ &#123; &quot;matchExpressions&quot; : [ &#123; &quot;key&quot; : &quot;storage&quot;, &quot;operator&quot; : &quot;In&quot;, &quot;values&quot; : [ &quot;glusterfs&quot; ] &#125; ] &#125; ] &#125; &#125; &#125;, &quot;hostNetwork&quot;: true, &quot;tolerations&quot; : [ &#123; &quot;key&quot; : &quot;node-role.kubernetes.io/storage&quot;, &quot;operator&quot; : &quot;Exists&quot;, &quot;effect&quot; : &quot;NoSchedule&quot; &#125; ], # kubectl create -f glusterfs-daemonset.json 配置heketi服务帐户创建集群角色绑定为服务帐户创建集群角色绑定，以授权控制gluster的pod 12# kubectl create -f heketi-service-account.json# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account 创建secret来保存Heketi服务的配置1kubectl create secret generic heketi-config-secret --from-file=./heketi.json 必须将配置文件heketi.json中的glusterfs/executor设置为kubernetes，如此才能让Heketi服务控制GlusterFS Pod。 Secret的名称空间，必须与gluserfs Pod位于同一名称空间内才能挂载之。 部署并验证一切正常运行1234~]# kubectl apply -f heketi-bootstrap.json~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEdeploy-heketi-858f965fd5-hcfq4 1/1 Running 0 3m58s 10.244.2.54 node02.isesol.local &lt;none&gt; 测试Heketi服务端既然Bootstrap Heketi服务正在运行，我们将配置端口转发，以便我们可以使用Heketi CLI与服务端进行通信。使用Heketi pod的名称，运行下面的命令： 123456[root@master01 kubernetes]# kubectl port-forward deploy-heketi-858f965fd5-hcfq4 8080:8080Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080[root@master01 ~]# curl http://localhost:8080/helloHello from Heketi 配置heketi与glusterfs1export HEKETI_CLI_SERVER=http://10.106.11.14:8080 #heketi的service的Cluster IP及Port 在示例文件中有个topology-sample.json文件，称为拓朴文件，它提供了运行gluster Pod的kubernetes节点IP，每个节点上相应的磁盘块设备，修改hostnames/manage，设置为与kubectl get nodes所显示的Name字段的值，通常为Node IP，修改hostnames/storage下的IP，为存储网络的IP地址，也即Node IP。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123[root@master01 tmp]# cat topology-sample.json &#123; &quot;clusters&quot;: [ &#123; &quot;nodes&quot;: [ &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage01.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.191&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage02.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.192&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage03.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.193&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage04.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.194&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage05.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.195&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage06.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.196&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125; ] &#125; ]&#125; 执行heketi-cli命令，导入拓扑 12345678910111213141516[root@master01 tmp]# ./heketi-cli topology load --json=topology-sample.jsonCreating cluster ... ID: fbb265f09857f98ad021368ba891562b Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node storage01.isesol.local ... ID: 185e10d74fb310c5d529112aa7b83eec Adding device /dev/sdb ... OK Creating node storage02.isesol.local ... ID: c17dc162dc6ca73a025cafdef2f49f66 Adding device /dev/sdb ... OK Creating node storage03.isesol.local ... ID: cb3e295cde4ead0077d22ba65215b2cb Adding device /dev/sdb ... OK Creating node storage04.isesol.local ... ID: f910638499d3a19403c45389c0605abf Adding device /dev/sdb ... OK Creating node storage05.isesol.local ... ID: c78f3d4047816e23282a6ec8d743bc58 Adding device /dev/sdb ... OK Creating node storage06.isesol.local ... ID: 8f8f3413d57993a9014c834180136573 Adding device /dev/sdb ... OK 配置Heketi数据持久 1234567891011121314151617181920212223242526272829303132333435363738~]# heketi-cli setup-openshift-heketi-storageSaving heketi-storage.json~]# kubectl create -f heketi-storage.jsonsecret/heketi-storage-secret createdendpoints/heketi-storage-endpoints createdservice/heketi-storage-endpoints createdjob.batch/heketi-storage-copy-job created#当heketi-storage-copy-job执行完成后，即可删除~]# kubectl get jobNAME COMPLETIONS DURATION AGEheketi-storage-copy-job 1/1 2s 2m#把之前由heketi-bootstrap.json创建的资源删除~]# kubectl delete all,service,jobs,deployment,secret --selector=&quot;deploy-heketi&quot;pod &quot;deploy-heketi-858f965fd5-hcfq4&quot; deletedservice &quot;deploy-heketi&quot; deleteddeployment.apps &quot;deploy-heketi&quot; deletedreplicaset.apps &quot;deploy-heketi-858f965fd5&quot; deletedjob.batch &quot;heketi-storage-copy-job&quot; deletedsecret &quot;heketi-storage-secret&quot; deleted~]# kubectl get podNAME READY STATUS RESTARTS AGEglusterfs-66nck 1/1 Running 0 43mglusterfs-86qc2 1/1 Running 0 43mglusterfs-ppzcq 1/1 Running 0 43mglusterfs-q5g87 1/1 Running 0 43mglusterfs-rcw82 1/1 Running 0 43mglusterfs-wn7jw 1/1 Running 0 43m~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheketi-storage-endpoints ClusterIP 10.104.184.13 &lt;none&gt; 1/TCP 3m49s 在执行kubectl create -f heketi-storage.json请确认Kubernetes的Node节点已经安装了glusterfs-fuse软件。 最后，使用heketi-deployment.json文件重新部署heketi 1234567891011121314151617181920212223242526272829303132333435363738# cat heketi-deployment.json··· &quot;volumeMounts&quot;: [ &#123; &quot;mountPath&quot;: &quot;/backupdb&quot;, &quot;name&quot;: &quot;heketi-db-secret&quot; &#125;, &#123; &quot;name&quot;: &quot;db&quot;, &quot;mountPath&quot;: &quot;/var/lib/heketi&quot; &#125;, &#123; &quot;name&quot;: &quot;config&quot;, &quot;mountPath&quot;: &quot;/etc/heketi&quot; &#125; ],...&quot;volumes&quot;: [ &#123; &quot;name&quot;: &quot;db&quot;, &quot;glusterfs&quot;: &#123; &quot;endpoints&quot;: &quot;heketi-storage-endpoints&quot;, &quot;path&quot;: &quot;heketidbstorage&quot; &#125; &#125;,...#~]# kubectl apply -f heketi-deployment.json secret/heketi-db-backup createdservice/heketi createddeployment.extensions/heketi created~]# kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEheketi 1 1 1 1 26s~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheketi ClusterIP 10.104.168.68 &lt;none&gt; 8080/TCP 68sheketi-storage-endpoints ClusterIP 10.104.184.13 &lt;none&gt; 1/TCP 13m 验证heketi是否在用在用gluster volume 1234567~]# kubectl get podNAME READY STATUS RESTARTS AGEheketi-754dfc7cdf-5gdcq 1/1 Running 0 11m~]# kubectl exec -ti heketi-754dfc7cdf-5gdcq -- /bin/shsh-4.4# mount | grep heketitmpfs on /etc/heketi type tmpfs (ro,relatime)172.20.40.191:heketidbstorage on /var/lib/heketi type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) 存储类使用创建类12345678910111213141516~]# cat gluster-storage-class.yaml apiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata: name: gluster-heketi annotations: storageclass.kubernetes.io/is-default-class: &quot;true&quot; # 集群的默认存储provisioner: kubernetes.io/glusterfsparameters: resturl: &quot;http://10.104.168.68:8080&quot; #heketi svc的ip:port restuser: &quot;admin&quot; restuserkey: &quot;My Secret Life&quot; #创建~]# kubectl apply -f gluster-storage-class.yaml 创建pvc和pv1234567891011121314151617181920212223~]# cat gluster-pvc-gvolume-test.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: gvolume-test annotations: volume.beta.kubernetes.io/storage-class: gluster-heketispec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ~]# kubectl apply -f gluster-pvc-gvolume-test.yaml persistentvolumeclaim/gvolume-test created~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEgvolume-test Bound pvc-9319d051-d1e7-11e8-ab5e-0050568faffd 1Gi RWO gluster-heketi 27s~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-9319d051-d1e7-11e8-ab5e-0050568faffd 1Gi RWO Delete Bound default/gvolume-test gluster-heketi 20s 使用Pvc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#Pod引用pv[root@master01 pod]# cat gvolume.yaml apiVersion: v1kind: Podmetadata: name: gvolume-test namespace: default labels: app: gvolumespec: containers: - name: gvolume-test image: tomcat:8.5.34-alpine volumeMounts: - mountPath: /usr/local/tomcat/webapps/ROOT name: gvolume-test volumes: - name: gvolume-test persistentVolumeClaim: claimName: gvolume-test [root@master01 pod]# kubectl apply -f gvolume.yaml pod/gvolume-test created#登陆Pod查看mount卷[root@master01 pod]# kubectl exec -ti gvolume-test -- /bin/sh/usr/local/tomcat # mount | grep glusterfs172.20.40.191:vol_930bd0a752f309da01c3e55e785e2d50 on /usr/local/tomcat/webapps/ROOT type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)#在卷中写入内容/usr/local/tomcat # cd /usr/local/tomcat/webapps/ROOT/usr/local/tomcat/webapps/ROOT # touch index.html/usr/local/tomcat/webapps/ROOT # echo &quot;hello glusterfs&quot; &gt;&gt; index.html #查看PV对应的Volume ID[root@master01 pod]# kubectl describe pv pvc-9319d051-d1e7-11e8-ab5e-0050568faffdName: pvc-9319d051-d1e7-11e8-ab5e-0050568faffdLabels: &lt;none&gt;Annotations: Description: Gluster-Internal: Dynamically provisioned PV gluster.kubernetes.io/heketi-volume-id: 930bd0a752f309da01c3e55e785e2d50 #hekeit的vg名称,即volume ID gluster.org/type: file kubernetes.io/createdby: heketi-dynamic-provisioner pv.beta.kubernetes.io/gid: 2000 pv.kubernetes.io/bound-by-controller: yes pv.kubernetes.io/provisioned-by: kubernetes.io/glusterfsFinalizers: [kubernetes.io/pv-protection]StorageClass: gluster-heketiStatus: BoundClaim: default/gvolume-testReclaim Policy: DeleteAccess Modes: RWOCapacity: 1GiNode Affinity: &lt;none&gt;Message: Source: Type: Glusterfs (a Glusterfs mount on the host that shares a pod&apos;s lifetime) EndpointsName: glusterfs-dynamic-gvolume-test Path: vol_930bd0a752f309da01c3e55e785e2d50 ReadOnly: falseEvents: &lt;none&gt;#通过heketi查看volume分布的服务器位置#通过VolumeID找到所对应的Node节点，即storage01.isesol.local节点[root@master01 ~]# kubectl exec -ti heketi-754dfc7cdf-5gdcq -- /bin/shsh-4.4# heketi-cli topology infoName: vol_930bd0a752f309da01c3e55e785e2d50 Size: 1 Id: 930bd0a752f309da01c3e55e785e2d50 Cluster Id: fbb265f09857f98ad021368ba891562b Mount: 172.20.40.191:vol_930bd0a752f309da01c3e55e785e2d50 Mount Options: backup-volfile-servers=172.20.40.196,172.20.40.192,172.20.40.195,172.20.40.193,172.20.40.194 Durability Type: replicate Replica: 3 Snapshot: Enabled Snapshot Factor: 1.00 Bricks: Id: 1adaa7d5c825c7f15c2d30fc9c723bb8 Path: /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/brick Size (GiB): 1 Node: 185e10d74fb310c5d529112aa7b83eec Device: f49b75bdfe1165d8340421459d4413f1 Id: 759b1139ce5ab60d2fc159514de42e33 Path: /var/lib/heketi/mounts/vg_c44590f9656ad0f015df01f2b3d206a5/brick_759b1139ce5ab60d2fc159514de42e33/brick Size (GiB): 1 Node: f910638499d3a19403c45389c0605abf Device: c44590f9656ad0f015df01f2b3d206a5 Id: dc6a7be99d9307e6494e5bbd96512fc1 Path: /var/lib/heketi/mounts/vg_bddab02435633f4ca495431032b552c4/brick_dc6a7be99d9307e6494e5bbd96512fc1/brick Size (GiB): 1 Node: c78f3d4047816e23282a6ec8d743bc58 Device: bddab02435633f4ca495431032b552c4 Nodes: Node Id: 185e10d74fb310c5d529112aa7b83eec State: online Cluster Id: fbb265f09857f98ad021368ba891562b Zone: 1 Management Hostnames: storage01.isesol.local Storage Hostnames: 172.20.40.191 Devices: Id:f49b75bdfe1165d8340421459d4413f1 Name:/dev/sdb State:online Size (GiB):499 Used (GiB):1 Free (GiB):498 Bricks: Id:1adaa7d5c825c7f15c2d30fc9c723bb8 Size (GiB):1 Path: /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/brick #登陆storage01.isesol.local glusterfs节点查看[root@master01 ~]# kubectl exec -ti glusterfs-wn7jw -- /bin/shsh-4.2# cd /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/bricksh-4.2# lsindex.htmlsh-4.2# cat index.html hello glusterfs Heketi维护查看节点 1234567sh-4.4# heketi-cli node listId:185e10d74fb310c5d529112aa7b83eec Cluster:fbb265f09857f98ad021368ba891562bId:8f8f3413d57993a9014c834180136573 Cluster:fbb265f09857f98ad021368ba891562bId:c17dc162dc6ca73a025cafdef2f49f66 Cluster:fbb265f09857f98ad021368ba891562bId:c78f3d4047816e23282a6ec8d743bc58 Cluster:fbb265f09857f98ad021368ba891562bId:cb3e295cde4ead0077d22ba65215b2cb Cluster:fbb265f09857f98ad021368ba891562bId:f910638499d3a19403c45389c0605abf Cluster:fbb265f09857f98ad021368ba891562b 查看volume 123sh-4.4# heketi-cli volume list Id:7eef8baa9a616e8e9db78fc71f862498 Cluster:fbb265f09857f98ad021368ba891562b Name:heketidbstorageId:930bd0a752f309da01c3e55e785e2d50 Cluster:fbb265f09857f98ad021368ba891562b Name:vol_930bd0a752f309da01c3e55e785e2d50 查看拓扑信息 1sh-4.4# heketi-cli topology info 查看集群信息123sh-4.4# heketi-cli cluster listClusters:Id:fbb265f09857f98ad021368ba891562b [file][block] 扩展集群节点 如果集群节点的复本数是3，需要同时添加3个节点，如果复本数是2，即需要同时添加两个节点 1234567891011121314151617181920212223$ heketi-cli node add \ --zone=3 \ --cluster=3e21671bc4f290fca6bce464ae7bb6e7 \ --management-host-name=node1-manage.gluster.lab.com \ --storage-host-name=172.18.10.53Node information:Id: e0017385b683c10e4166492e78832d09State: onlineCluster Id: 3e21671bc4f290fca6bce464ae7bb6e7Zone: 3Management Hostname node1-manage.gluster.lab.comStorage Hostname 172.18.10.53$ heketi-cli device add \ --name=/dev/sdb \ --node=e0017385b683c10e4166492e78832d09Device added successfully$ heketi-cli device add \ --name=/dev/sdc \ --node=e0017385b683c10e4166492e78832d09Device added successfully 常见错误12345Adding device /dev/vdb ... Unable to add device: Unable to execute command on glusterfs-xp1nx: Can&apos;t initialize physical volume &quot;/dev/vdb&quot;of volume group &quot;vg_dc649bdf755667e58c5d779f9d900057&quot; without -ffdd if=/dev/zero of=/dev/vdb bs=1k count=1blockdev --rereadpt /dev/vdb 删除gluster组件123451、删除各gluster和heketi相关的k8s资源2、在各存储node上，初始化相关磁盘3、各节点上删除如下目录： $ rm -rf /var/lib/heketi $ rm -rf /var/lib/glusterd 向集群添加host解析12345678910111213141516171819202122232425262728293031323334353637~]# kubectl edit configmap coredns -n kube-systemapiVersion: v1data: Corefile: | .:53 &#123; errors health hosts &#123; #添加所需要解析的域名和主机即可 172.20.40.11 k8s-master01 172.20.40.12 k8s-master02 172.20.40.13 k8s-master03 172.20.40.15 k8s-node01 172.20.40.16 k8s-node02 172.20.40.17 k8s-node03 172.20.40.18 k8s-node04 172.20.40.19 k8s-node05 172.20.40.11 api.isesoldev.com fallthrough &#125; kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload &#125;kind: ConfigMapmetadata: creationTimestamp: 2018-10-31T13:05:49Z name: coredns namespace: kube-system resourceVersion: &quot;115223&quot; selfLink: /api/v1/namespaces/kube-system/configmaps/coredns uid: b00382f4-dd0d-11e8-9f47-0050568f5598]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
</search>
