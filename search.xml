<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker registry删除镜像方法]]></title>
    <url>%2F2018%2F09%2F20%2Fdocker%2F</url>
    <content type="text"><![CDATA[docker-distribution主配置文件123456789101112131415161718192021config.yml: version: 0.1 log: fields: service: registry storage: delete: enabled: true #需要开启存储删除功能，才能使用接口删除镜像。 cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registry http: addr: :5000 headers: X-Content-Type-Options: [nosniff] health: storagedriver: enabled: true interval: 10s threshold: 3 删除Image的步骤 1、获取Etag字段中的sha256的字段 12345678910curl -i -X GET --header &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; http://localhost:5000/v2/镜像名称/manifests/标签 HTTP/1.1 200 OK Content-Length: 3047 Content-Type: application/vnd.docker.distribution.manifest.v2+json Docker-Content-Digest: sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 Docker-Distribution-Api-Version: registry/2.0 Etag: &quot;sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035&quot; #获取Etag字段的VaLue,sha256:xxx字段。 X-Content-Type-Options: nosniff Date: Wed, 11 Apr 2018 05:51:24 GMT 2、根据Etag字段来删除标签 123456789101112131415161718curl -k -v -s -X DELETE http://localhost:5000/v2/tomcat/manifests/ETAG * About to connect() to localhost port 5000 (#0) * Trying 127.0.0.1... * Connected to localhost (127.0.0.1) port 5000 (#0) &gt; DELETE /v2/tomcat/manifests/sha256:4111f52442b430fa1c758156cb20dd7884dbde3fcbc969f166850d9f6da06035 HTTP/1.1 &gt; User-Agent: curl/7.29.0 &gt; Host: localhost:5000 &gt; Accept: */* &gt; &lt; HTTP/1.1 202 Accepted #返回202 Accepted内容，即表示删除成功 &lt; Docker-Distribution-Api-Version: registry/2.0 &lt; X-Content-Type-Options: nosniff &lt; Date: Wed, 11 Apr 2018 05:56:21 GMT &lt; Content-Length: 0 &lt; Content-Type: text/plain; charset=utf-8 &lt; * Connection #0 to host localhost left intact 3、获取标签，以确认删除标签成功 12curl -X GET http://localhost:5000/v2/tomcat/tags/list &#123;&quot;name&quot;:&quot;ubuntu&quot;,&quot;tags&quot;:null&#125; 4、释放磁盘空间 12345678910# registry garbage-collect /etc/docker-distribution/registry/config.yml 10 blobs marked, 15 blobs eligible for deletion #返回10个blobs被标记，15个适合删除的块。blob eligible for deletion: sha256:68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/68/68fb305098765361009a270c1e5293e3972d008761ee30260e8797adf950a830 go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09 #INFO信息中，返回的是: 块所对应的文件路径blob eligible for deletion:sha256:96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecdINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/96/96e1d8c6601afdc4b27ce3e6bb9c9460e788cb5f66a6cc71255e88a004071ecd go.version=go1.8.3 instance.id=ffccf012-55fc-440d-8820-9f8c7393ad09blob eligible for deletion: sha256:0b54f3727113712ee413d614a38f2a7e25c568d7c951c56487a90d4434426646## curl -X GET http://localhost:5000/v2/_catalog : 获取镜像的名称列表当删除镜像后，需要重新上传时，遇到错误: Layer already exists, 需要重启docker-distribution服务即可。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubernetes]]></title>
    <url>%2F2018%2F09%2F19%2Fkubernetes%2F</url>
    <content type="text"><![CDATA[安装harbor服务Harbor 安装前准备安装docker12345678# yum install -y yum-utils device-mapper-persistent-data lvm2# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# yum -y install docker-ce# docker --versionDocker version 17.06.2-ce, build cec0b72# systemctl start docker# systemctl status docker# systemctl enable docker 安装docker-compose1234# yum -y install python-pip# pip install --upgrade pip# pip install docker-compose# docker-compose version 安装 Habor12345678910111213141516171819# wget https://storage.googleapis.com/harbor-releases/release-1.6.0/harbor-offline-installer-v1.6.0.tgz# tar xf harbor-offline-installer-v1.6.0.tgz# cd harbor# docker load -i harbor.v1.6.0.tar.gz# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgoharbor/chartmuseum-photon v0.7.1-v1.6.0 99bfb7b3aa9c 4 weeks ago 357MBgoharbor/harbor-migrator v1.6.0 23ed5c5918a0 4 weeks ago 803MBgoharbor/redis-photon v1.6.0 745667dc5aa8 4 weeks ago 214MBgoharbor/clair-photon v2.0.5-v1.6.0 01cb5fff1728 4 weeks ago 308MBgoharbor/notary-server-photon v0.5.1-v1.6.0 11dfd338b15c 4 weeks ago 215MBgoharbor/notary-signer-photon v0.5.1-v1.6.0 08436cc747a3 4 weeks ago 212MBgoharbor/registry-photon v2.6.2-v1.6.0 1ec7d8d4f0fd 4 weeks ago 201MBgoharbor/nginx-photon v1.6.0 81df0f8a78c0 4 weeks ago 138MBgoharbor/harbor-log v1.6.0 0f474b9d4565 4 weeks ago 203MBgoharbor/harbor-jobservice v1.6.0 4e6a3afe6802 4 weeks ago 198MBgoharbor/harbor-ui v1.6.0 9cf3894e769e 4 weeks ago 221MBgoharbor/harbor-adminserver v1.6.0 14d9ee1bbda3 4 weeks ago 187MBgoharbor/harbor-db v1.6.0 5c39f18ce348 4 weeks ago 225MB 配置harbor12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# sed -i &quot;s/reg.mydomain.com/registry01.isesol.local/g&quot; harbor.cfg# grep ^[a-z] harbor.cfg hostname = registry01.isesol.localui_url_protocol = httpmax_job_workers = 10 customize_crt = onssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.keysecretkey_path = /dataadmiral_url = NAlog_rotate_count = 50log_rotate_size = 200Mhttp_proxy =https_proxy =no_proxy = 127.0.0.1,localhost,ui,registryemail_identity = email_server = smtp.mydomain.comemail_server_port = 25email_username = admin@mydomain.comemail_password = abcemail_from = admin &lt;admin@mydomain.com&gt;email_ssl = falseemail_insecure = falseharbor_admin_password = 12345678auth_mode = ldap_authldap_url = ldap://172.20.10.61:389ldap_searchdn = uid=confluence,ou=users,dc=isesol,dc=localldap_search_pwd = 12345678ldap_basedn = ou=users,dc=isesol,dc=localldap_filter = (objectClass=person)ldap_uid = uidldap_scope = 3 ldap_timeout = 5ldap_verify_cert = trueldap_group_basedn = ou=users,dc=isesol,dc=localldap_group_filter = objectclass=groupldap_group_gid = cnldap_group_scope = 2self_registration = ontoken_expiration = 30project_creation_restriction = everyonedb_host = postgresqldb_password = root123db_port = 5432db_user = postgresredis_host = redisredis_port = 6379redis_password = redis_db_index = 1,2,3clair_db_host = postgresqlclair_db_password = root123clair_db_port = 5432clair_db_username = postgresclair_db = postgresclair_updaters_interval = 12uaa_endpoint = uaa.mydomain.orguaa_clientid = iduaa_clientsecret = secretuaa_verify_cert = trueuaa_ca_cert = /path/to/ca.pemregistry_storage_provider_name = filesystemregistry_storage_provider_config =registry_custom_ca_bundle = 注:harbor 的主机名 hostname 不能注释再指定，必须删除默认设置再指定主机名，不然会产生错误。 安装 harbor1# ./install.sh 查看容器状况1234567891011# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------------------------------------harbor-adminserver /harbor/start.sh Up (healthy) harbor-db /entrypoint.sh postgres Up (healthy) 5432/tcp harbor-jobservice /harbor/start.sh Up harbor-log /bin/sh -c /usr/local/bin/ ... Up (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-ui /harbor/start.sh Up (healthy) nginx nginx -g daemon off; Up (healthy) 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcpredis docker-entrypoint.sh redis ... Up 6379/tcp registry /entrypoint.sh /etc/regist ... Up (healthy) 5000/tcp Harbor共由七个容器组成: harbor-adminserver:harbor系统管理服务 harbor-db: 由官方mysql镜像构成的数据库容器 harbor-jobservice:harbor的任务管理服务 harbor-log:harbor的日志收集、管理服务 harbor-ui:harbor的web页面服务 nginx:负责流量转发和安全验证 registry:官方的Docker registry，负责保存镜像 更改 docker 配置12345[root@registry01 harbor]# cat /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], #加速docker &quot;insecure-registries&quot;: [&quot;registry01.isesol.local&quot;] #加入非安全访问服务器&#125; 登陆 harbor1234567# docker login -u admin -p 12345678 registry01.isesol.localLogin Succeeded# docker login registry01.isesol.localUsername: adminPassword: Login Succeeded harbor服务管理12345678停止harbor# docker-compose stop启动harbor# docker-compose start重新Prepare# docker-compose down -v# ./prepare# docker-compose up -d harbor日志存放位置123456789# ll /var/log/harbor/-rw-r----- 1 10000 10000 602211 10月 10 11:08 adminserver.log-rw-r----- 1 10000 10000 46360 10月 9 18:06 jobservice.log-rw-r----- 1 10000 10000 25644 10月 9 17:07 mysql.log-rw-r----- 1 10000 10000 63488 10月 9 18:06 postgresql.log-rw-r----- 1 10000 10000 410516 10月 10 11:08 proxy.log-rw-r----- 1 10000 10000 160657 10月 10 11:05 redis.log-rw-r----- 1 10000 10000 319761 10月 10 11:08 registry.log-rw-r----- 1 10000 10000 1078369 10月 10 11:08 ui.log harbor ldap配置 Kubernetes安装环境说明123456789master01: 172.20.40.200master02: 172.20.40.201master03: 172.20.40.202node01: 172.20.40.203node02: 172.20.40.204node03: 172.20.40.205harbor: 172.20.40.199pod-network-cidr: 10.244.0.0/16 service-cidr: 10.96.0.0/12 非集群安装master配置yum仓库12345678910111213# cd /etc/yum.repos.d/# cat aliyun-k8s.repo [kubernetes-aliyun] name=aliyun kubernetes repo baseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot; gpgcheck=0 enabled=1# cat aliyun-docker.repo [aliyun-docker] name=aliyun docker baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/ enabled=1 gpgcheck=0 安装master节点关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# master01.isesol.local节点# yum install docker-ce kubelet kubeadm kubectl# systemctl start docker# systemctl enable kubelet# systemctl enable docker# 手动获取依赖镜像 #获取相应的images列表及版本 [root@master01 ~]# kubeadm config images list k8s.gcr.io/kube-apiserver:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 k8s.gcr.io/pause:3.1 k8s.gcr.io/etcd:3.2.24 k8s.gcr.io/coredns:1.2.2 #从docker hub拉取镜像到本地 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-apiserver:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-controller-manager:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-scheduler:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@master01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@master01 ~]# docker pull mirrorgooglecontainers/etcd:3.2.24 [root@master01 ~]# docker pull coredns/coredns:1.2.2 # 重新打tag [root@master01 ~]# docker tag mirrorgooglecontainers/kube-apiserver:v1.12.1 k8s.gcr.io/kube-apiserver:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-controller-manager:v1.12.1 k8s.gcr.io/kube-controller-manager:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-scheduler:v1.12.1 k8s.gcr.io/kube-scheduler:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@master01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@master01 ~]# docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24 [root@master01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2#初始化安装[root@master01 ~]# kubeadm init --kubernetes-version=v1.12.1 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 [init] using Kubernetes version: v1.12.1 [preflight] running pre-flight checks [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [preflight] Activating the kubelet service [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [master01.isesol.local localhost] and IPs [172.20.40.200 127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] Generated ca certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [master01.isesol.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.20.40.200] [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [certificates] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot; [kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 25.003180 seconds [uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster [markmaster] Marking the node master01.isesol.local as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot; [markmaster] Marking the node master01.isesol.local as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;master01.isesol.local&quot; as an annotation [bootstraptoken] using token: 4sjr1s.2agrbsj96c4hc17c [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc# 配置集群管理员 [root@master01 ~]# mkdir -p $HOME/.kube [root@master01 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master01 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看安装状态 [root@master01 manifests]# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-cncg6 0/1 Pending 0 5m13s coredns-576cbf47c7-cx9bg 0/1 Pending 0 5m13s etcd-master01.isesol.local 1/1 Running 0 4m25s kube-apiserver-master01.isesol.local 1/1 Running 0 4m20s kube-controller-manager-master01.isesol.local 1/1 Running 0 4m32s kube-proxy-n7bl5 1/1 Running 0 5m13s kube-scheduler-master01.isesol.local 1/1 Running 0 4m32s &gt; 此处coredns未ready, 查看报错信息为： 0/1 nodes are available: 1 node(s) had taints that the pod didn&apos;t tolerate，需要添加node节点 安装CNI12# docker pull quay.io/coreos/flannel:v0.10.0-amd64# apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 安装Node节点 在node01.isesol.local节点安装 关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 关闭防火墙及Selinux12345678# systemctl stop firewalld.service# systemctl disable firewalld.service# setenforce 0# cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 安装服务1234567891011[root@node01 ~]# yum install docker-ce kubelet kubeadm[root@node01 ~]# systemctl start docker[root@node01 ~]# systemctl enable docker kubelet# 准备镜像 [root@node01 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 [root@node01 ~]# docker pull mirrorgooglecontainers/pause:3.1 [root@node01 ~]# docker pull coredns/coredns:1.2.2 [root@node01 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 [root@node01 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 [root@node01 ~]# docker tag coredns/coredns:1.2.2 k8s.gcr.io/coredns:1.2.2 节点加入集群123456789101112131415161718192021222324[root@node01 ~]# kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc [preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;] you can solve this problem with following methods: 1. Run &apos;modprobe -- &apos; to load missing kernel modules; 2. Provide the missing builtin kernel ipvs support [discovery] Trying to connect to API Server &quot;172.20.40.200:6443&quot; [discovery] Created cluster-info discovery client, requesting info from &quot;https://172.20.40.200:6443&quot; [discovery] Requesting info from &quot;https://172.20.40.200:6443&quot; again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.20.40.200:6443&quot; [discovery] Successfully established connection with API Server &quot;172.20.40.200:6443&quot; [kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot; [kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot; [preflight] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;node01.isesol.local&quot; as an annotation This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run &apos;kubectl get nodes&apos; on the master to see this node join the cluster. 安装dashborad123456789101112131415161718192021# 在Node节点上准备镜像 # docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 # docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10# 在Master节点下安装Dashboard # wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml # vim kubernetes-dashboard.yaml kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort #添加此选项 配置dashboard的Token认证123456789101112131415161718192021222324252627282930313233# 创建dashboard-admin的ServiceAccountkubectl create serviceaccount dashboard-admin -n kube-system # 创建dashboard-cluster-admin的clusterrolebding，并绑定cluster-admin的clusterrole（集群管理员),关联kube-system空间下的dashboard-admin的Sakubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secret dashboad-admin-token-XXX -n kube-system #查询serviceaccount对应的secret # kubectl describe sa dashboard-admin -n kube-system Name: dashboard-admin Namespace: kube-system Labels: &lt;none&gt; Annotations: &lt;none&gt; Image pull secrets: &lt;none&gt; Mountable secrets: dashboard-admin-token-8zb56 Tokens: dashboard-admin-token-8zb56 #此tonken为sa绑定的secret Events: &lt;none&gt;# 获取secret的Token信息 # kubectl describe secret dashboard-admin-token-8zb56 -n kube-system Name: dashboard-admin-token-8zb56 Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 10a3b5d2-cc5d-11e8-ab5e-0050568faffd Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes #使用token登陆 token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tOHpiNTYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTBhM2I1ZDItY2M1ZC0xMWU4LWFiNWUtMDA1MDU2OGZhZmZkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.Pu8meO-UBK4F65sU2i56I01EnqP5Btzfl3eyNllb3kagLudEvFIkkpc5RpawsBlo4yzUFWT1HY50QlJKMbcJyHo7__O-sLkg88lLN_d9lX1usGYE2udMW112uLc52Y67e14Ni9KAju9RrvSCeCg-8PeR83k5f3Zq4ZetAXotSG0_NIkPQdgYP6jSmV3hugCHuk9-Cr0E5uSpr4-LcN81SEbLOwe0ar70K3JgkC7RwFpSRt-GuF8iUn-KZaBOqFAkIDSSc9aVhptgJ1b7dkSlIu_fT9yNq6p_BJ5RzwN3DJteY2POVitmKNivHCWD7K2ubo_XzimruIxVPj3qEkoLow 配置dashboard的kubeconfig认证kubeconfig支持两种类型绑定： Serviceaccount Useraccount 本节演示基于serviceaccount绑定认证，并使用Token认证中的Serviceaccount实现权限需要根据Serviceaccount的rolebinding实现此处只是将serviceaccount封装成了kubeconfig配置文件的形式 1234567# cd /etc/kubernetes/pki/# kubectl config set-cluster kubernetes --certificate-authority=./ca.crt --server=&quot;http://172.20.40.200:6443&quot; --embed-certs=true --kubeconfig=/root/cluster-admin.conf# kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; #需要找到serviceaccount对应的secret中的token字段# TOKENID=$(kubectl get secret dashboard-admin-token-8zb56 -n kube-system -o jsonpath=&#123;.data.token&#125; | base64 -d) #使用base64解码并赋值给变量TOKENID# kubectl config set-credentials cluster-admin --token=$TOKENID --kubeconfig=/root/cluster-admin.conf# kubectl config set-context cluster-admin@kubernetes --cluster=kubernetes --user=cluster-admin --kubeconfig=/root/cluster-admin.conf# kubectl config use-context cluster-admin@kubernetes --kubeconfig=/root/cluster-admin.conf 生成kubeconfig文件的步骤 1234kubectl config set-cluster --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-credentials NAME --token=$KUBE_TOKEN --kubeconfig=/PATH/TO/SOMEFILEkubectl config set-context kubectl config use-context Glusterfs环境准备 环境说明 123456172.20.40.191 storage01.isesol.local storage01172.20.40.192 storage02.isesol.local storage02172.20.40.193 storage03.isesol.local storage03172.20.40.194 storage04.isesol.local storage04172.20.40.195 storage05.isesol.local storage05172.20.40.196 storage06.isesol.local storage06 配置时间同步 12~]# cat /etc/cron.d/timesync 00 */1 * * * /usr/sbin/ntpdate ntp.api.bz &amp;&gt; /dev/nul 配置各节点免密码登陆 各节点准备存储磁盘 12345~]# fdisk -l /dev/sdb 磁盘 /dev/sdb：536.9 GB, 536870912000 字节，1048576000 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节 关闭firewalld服务 12systemctl stop firewalld.servicesystemctl disable firewalld.service 存储节点加入集群配置yum仓库123456789101112131415161718192021222324252627282930[aliyun-docker]name=aliyun dockerbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/enabled=1gpgcheck=0[kubernetes-aliyun]name=aliyun kubernetes repobaseurl=&quot;https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&quot;gpgcheck=0enabled=1[base]name=aliyun base repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/os/x86_64/&quot;gpgcheck=0enabled=1[extras]name=aliyun extras repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/extras/x86_64/&quot;gpgcheck=0enabled=1[updates]name=aliyun updates repobaseurl=&quot;https://mirrors.aliyun.com/centos/7/updates/x86_64/&quot;gpgcheck=0enabled=1[epel]name=apiyun epel repobaseurl=&quot;https://mirrors.aliyun.com/epel/7Server/x86_64/&quot;gpgcheck=0enabled=1 关闭swap分区12# swapoff -a注释fstab文件中的swap挂载 安装服务123456789~]# yum install docker-ce kubelet kubeadm~]# systemctl start docker~]# systemctl enable docker kubelet # 准备镜像到本地 ~]# docker pull mirrorgooglecontainers/kube-proxy:v1.12.1 ~]# docker pull mirrorgooglecontainers/pause:3.1 ~]# docker tag mirrorgooglecontainers/kube-proxy:v1.12.1 k8s.gcr.io/kube-proxy:v1.12.1 ~]# docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 节点加入集群1kubeadm join 172.20.40.200:6443 --token q1lu4f.8v3frs2mlvxmy0r5 --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc token TTL由于token的TTL默认是24个小时，如果超出这个时间，使用过期的token加入集群，会报如下错误：123456789101112131415161718192021222324252627282930[root@storage01 kubernetes]# kubeadm join 172.20.40.200:6443 --token 4sjr1s.2agrbsj96c4hc17c --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc [preflight] running pre-flight checks [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin kernel ipvs support: map[ip_vs:&#123;&#125; ip_vs_rr:&#123;&#125; ip_vs_wrr:&#123;&#125; ip_vs_sh:&#123;&#125; nf_conntrack_ipv4:&#123;&#125;]you can solve this problem with following methods: 1. Run &apos;modprobe -- &apos; to load missing kernel modules;2. Provide the missing builtin kernel ipvs support[discovery] Trying to connect to API Server &quot;172.20.40.200:6443&quot;[discovery] Created cluster-info discovery client, requesting info from &quot;https://172.20.40.200:6443&quot;[discovery] Requesting info from &quot;https://172.20.40.200:6443&quot; again to validate TLS against the pinned public key[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;172.20.40.200:6443&quot;[discovery] Successfully established connection with API Server &quot;172.20.40.200:6443&quot;[kubelet] Downloading configuration for the kubelet from the &quot;kubelet-config-1.12&quot; ConfigMap in the kube-system namespaceUnauthorized#查看token的状态 ~]# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS4sjr1s.2agrbsj96c4hc17c &lt;invalid&gt; 2018-10-11T13:01:14+08:00 authentication,signing The default bootstrap token generated by &apos;kubeadm init&apos;. system:bootstrappers:kubeadm:default-node-token# 需要手动创建一个新的Token~]# kubeadm token create~]# kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSq1lu4f.8v3frs2mlvxmy0r5 23h 2018-10-17T12:11:04+08:00 authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token#使用新的token加入集群kubeadm join 172.20.40.200:6443 --token q1lu4f.8v3frs2mlvxmy0r5 --discovery-token-ca-cert-hash sha256:1a5ad9affde48a2482af79636ea561cefb8b11b51b82f78aca1f89f440b71fcc 查看node节点状态1234567891011[root@master01 pki]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster01.isesol.local Ready master 5d23h v1.12.1node01.isesol.local Ready &lt;none&gt; 5d23h v1.12.1node02.isesol.local Ready &lt;none&gt; 5d22h v1.12.1storage01.isesol.local Ready &lt;none&gt; 10m v1.12.1storage02.isesol.local Ready &lt;none&gt; 9m27s v1.12.1storage03.isesol.local Ready &lt;none&gt; 9m24s v1.12.1storage04.isesol.local Ready &lt;none&gt; 9m20s v1.12.1storage05.isesol.local Ready &lt;none&gt; 9m17s v1.12.1storage06.isesol.local Ready &lt;none&gt; 8m18s v1.12.1 配置存储节点添加taints123456kubectl taint node storage01.isesol.local storage=glusterfs:NoSchedulekubectl taint node storage02.isesol.local storage=glusterfs:NoSchedulekubectl taint node storage03.isesol.local storage=glusterfs:NoSchedulekubectl taint node storage04.isesol.local storage=glusterfs:NoSchedulekubectl taint node storage05.isesol.local storage=glusterfs:NoSchedulekubectl taint node storage06.isesol.local storage=glusterfs:NoSchedule 添加标签和taint12345678910111213141516kubectl taint node storage01.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage02.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage03.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage04.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage05.isesol.local node-role.kubernetes.io/storage=:NoSchedulekubectl taint node storage06.isesol.local node-role.kubernetes.io/storage=:NoSchedule#验证配置[root@master01 ~]# kubectl get node -l storage=glusterfsNAME STATUS ROLES AGE VERSIONstorage01.isesol.local Ready &lt;none&gt; 57m v1.12.1storage02.isesol.local Ready &lt;none&gt; 55m v1.12.1storage03.isesol.local Ready &lt;none&gt; 55m v1.12.1storage04.isesol.local Ready &lt;none&gt; 55m v1.12.1storage05.isesol.local Ready &lt;none&gt; 55m v1.12.1storage06.isesol.local Ready &lt;none&gt; 54m v1.12.1 glusterfs和Heketi 借鉴的技术文章：http://blog.51cto.com/newfly 下载1git clone https://github.com/heketi/heketi.git 配置存储节点12345modprobe rdma_cmmodprobe dm_thin_poolecho &quot;modprobe rdma_cm&quot; &gt;&gt; /etc/rc.localecho &quot;modprobe dm_thin_pool&quot; &gt;&gt; /etc/rc.localchmod +x /etc/rc.local 配置gluster daemonset123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354cd heketi/extras/kubernetes/vim glusterfs-daemonse.json&#123; &quot;kind&quot;: &quot;DaemonSet&quot;, &quot;apiVersion&quot;: &quot;apps/v1&quot;, &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;glusterfs&quot;, &quot;annotations&quot;: &#123; &quot;description&quot;: &quot;GlusterFS Daemon Set&quot;, &quot;tags&quot;: &quot;glusterfs&quot; &#125; &#125;, &quot;spec&quot;: &#123; &quot;selector&quot; : &#123; &quot;matchLabels&quot; : &#123; &quot;storage&quot;: &quot;glusterfs&quot;, &quot;glusterfs-node&quot;: &quot;daemonset&quot; &#125; &#125;, &quot;template&quot;: &#123; &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;glusterfs&quot;, &quot;labels&quot;: &#123; &quot;storage&quot;: &quot;glusterfs&quot; &#125; &#125;, &quot;spec&quot;: &#123; &quot;affinity&quot;: &#123; &quot;nodeAffinity&quot; : &#123; &quot;requiredDuringSchedulingIgnoredDuringExecution&quot; : &#123; &quot;nodeSelectorTerms&quot; : [ &#123; &quot;matchExpressions&quot; : [ &#123; &quot;key&quot; : &quot;storage&quot;, &quot;operator&quot; : &quot;In&quot;, &quot;values&quot; : [ &quot;glusterfs&quot; ] &#125; ] &#125; ] &#125; &#125; &#125;, &quot;hostNetwork&quot;: true, &quot;tolerations&quot; : [ &#123; &quot;key&quot; : &quot;node-role.kubernetes.io/storage&quot;, &quot;operator&quot; : &quot;Exists&quot;, &quot;effect&quot; : &quot;NoSchedule&quot; &#125; ], 配置heketi服务帐户创建集群角色绑定为服务帐户创建集群角色绑定，以授权控制gluster的pod 1kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account 创建secret来保存Heketi服务的配置1kubectl create secret generic heketi-config-secret --from-file=./heketi.json 必须将配置文件heketi.json中的glusterfs/executor设置为kubernetes，如此才能让Heketi服务控制GlusterFS Pod。 Secret的名称空间，必须与gluserfs Pod位于同一名称空间内才能挂载之。 部署并验证一切正常运行1234~]# kubectl apply -f heketi-bootstrap.json~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEdeploy-heketi-858f965fd5-hcfq4 1/1 Running 0 3m58s 10.244.2.54 node02.isesol.local &lt;none&gt; 测试Heketi服务端既然Bootstrap Heketi服务正在运行，我们将配置端口转发，以便我们可以使用Heketi CLI与服务端进行通信。使用Heketi pod的名称，运行下面的命令： 123456[root@master01 kubernetes]# kubectl port-forward deploy-heketi-858f965fd5-hcfq4 8080:8080Forwarding from 127.0.0.1:8080 -&gt; 8080Forwarding from [::1]:8080 -&gt; 8080[root@master01 ~]# curl http://localhost:8080/helloHello from Heketi 配置heketi与glusterfs1export HEKETI_CLI_SERVER=http://10.106.11.14:8080 #heketi的service的Cluster IP及Port 在示例文件中有个topology-sample.json文件，称为拓朴文件，它提供了运行gluster Pod的kubernetes节点IP，每个节点上相应的磁盘块设备，修改hostnames/manage，设置为与kubectl get nodes所显示的Name字段的值，通常为Node IP，修改hostnames/storage下的IP，为存储网络的IP地址，也即Node IP。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123[root@master01 tmp]# cat topology-sample.json &#123; &quot;clusters&quot;: [ &#123; &quot;nodes&quot;: [ &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage01.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.191&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage02.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.192&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage03.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.193&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage04.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.194&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage05.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.195&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125;, &#123; &quot;node&quot;: &#123; &quot;hostnames&quot;: &#123; &quot;manage&quot;: [ &quot;storage06.isesol.local&quot; ], &quot;storage&quot;: [ &quot;172.20.40.196&quot; ] &#125;, &quot;zone&quot;: 1 &#125;, &quot;devices&quot;: [ &#123; &quot;name&quot;: &quot;/dev/sdb&quot;, &quot;destroydata&quot;: false &#125; ] &#125; ] &#125; ]&#125; 执行heketi-cli命令，导入拓扑 12345678910111213141516[root@master01 tmp]# ./heketi-cli topology load --json=topology-sample.jsonCreating cluster ... ID: fbb265f09857f98ad021368ba891562b Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node storage01.isesol.local ... ID: 185e10d74fb310c5d529112aa7b83eec Adding device /dev/sdb ... OK Creating node storage02.isesol.local ... ID: c17dc162dc6ca73a025cafdef2f49f66 Adding device /dev/sdb ... OK Creating node storage03.isesol.local ... ID: cb3e295cde4ead0077d22ba65215b2cb Adding device /dev/sdb ... OK Creating node storage04.isesol.local ... ID: f910638499d3a19403c45389c0605abf Adding device /dev/sdb ... OK Creating node storage05.isesol.local ... ID: c78f3d4047816e23282a6ec8d743bc58 Adding device /dev/sdb ... OK Creating node storage06.isesol.local ... ID: 8f8f3413d57993a9014c834180136573 Adding device /dev/sdb ... OK 配置Heketi数据持久 1234567891011121314151617181920212223242526272829303132333435363738~]# heketi-cli setup-openshift-heketi-storageSaving heketi-storage.json~]# kubectl create -f heketi-storage.jsonsecret/heketi-storage-secret createdendpoints/heketi-storage-endpoints createdservice/heketi-storage-endpoints createdjob.batch/heketi-storage-copy-job created#当heketi-storage-copy-job执行完成后，即可删除~]# kubectl get jobNAME COMPLETIONS DURATION AGEheketi-storage-copy-job 1/1 2s 2m#把之前由heketi-bootstrap.json创建的资源删除~]# kubectl delete all,service,jobs,deployment,secret --selector=&quot;deploy-heketi&quot;pod &quot;deploy-heketi-858f965fd5-hcfq4&quot; deletedservice &quot;deploy-heketi&quot; deleteddeployment.apps &quot;deploy-heketi&quot; deletedreplicaset.apps &quot;deploy-heketi-858f965fd5&quot; deletedjob.batch &quot;heketi-storage-copy-job&quot; deletedsecret &quot;heketi-storage-secret&quot; deleted~]# kubectl get podNAME READY STATUS RESTARTS AGEglusterfs-66nck 1/1 Running 0 43mglusterfs-86qc2 1/1 Running 0 43mglusterfs-ppzcq 1/1 Running 0 43mglusterfs-q5g87 1/1 Running 0 43mglusterfs-rcw82 1/1 Running 0 43mglusterfs-wn7jw 1/1 Running 0 43m~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheketi-storage-endpoints ClusterIP 10.104.184.13 &lt;none&gt; 1/TCP 3m49s 在执行kubectl create -f heketi-storage.json请确认Kubernetes的Node节点已经安装了glusterfs-fuse软件。 最后，使用heketi-deployment.json文件重新部署heketi 1234567891011121314151617181920212223242526272829303132333435363738# cat heketi-deployment.json··· &quot;volumeMounts&quot;: [ &#123; &quot;mountPath&quot;: &quot;/backupdb&quot;, &quot;name&quot;: &quot;heketi-db-secret&quot; &#125;, &#123; &quot;name&quot;: &quot;db&quot;, &quot;mountPath&quot;: &quot;/var/lib/heketi&quot; &#125;, &#123; &quot;name&quot;: &quot;config&quot;, &quot;mountPath&quot;: &quot;/etc/heketi&quot; &#125; ],...&quot;volumes&quot;: [ &#123; &quot;name&quot;: &quot;db&quot;, &quot;glusterfs&quot;: &#123; &quot;endpoints&quot;: &quot;heketi-storage-endpoints&quot;, &quot;path&quot;: &quot;heketidbstorage&quot; &#125; &#125;,...#~]# kubectl apply -f heketi-deployment.json secret/heketi-db-backup createdservice/heketi createddeployment.extensions/heketi created~]# kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEheketi 1 1 1 1 26s~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheketi ClusterIP 10.104.168.68 &lt;none&gt; 8080/TCP 68sheketi-storage-endpoints ClusterIP 10.104.184.13 &lt;none&gt; 1/TCP 13m 验证heketi是否在用在用gluster volume 1234567~]# kubectl get podNAME READY STATUS RESTARTS AGEheketi-754dfc7cdf-5gdcq 1/1 Running 0 11m~]# kubectl exec -ti heketi-754dfc7cdf-5gdcq -- /bin/shsh-4.4# mount | grep heketitmpfs on /etc/heketi type tmpfs (ro,relatime)172.20.40.191:heketidbstorage on /var/lib/heketi type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) 存储类使用创建类12345678910111213141516~]# cat gluster-storage-class.yaml apiVersion: storage.k8s.io/v1beta1kind: StorageClassmetadata: name: gluster-heketi annotations: storageclass.kubernetes.io/is-default-class: &quot;true&quot; # 集群的默认存储provisioner: kubernetes.io/glusterfsparameters: resturl: &quot;http://10.104.168.68:8080&quot; #heketi svc的ip:port restuser: &quot;admin&quot; restuserkey: &quot;My Secret Life&quot; #创建~]# kubectl apply -f gluster-storage-class.yaml 创建pvc和pv1234567891011121314151617181920212223~]# cat gluster-pvc-gvolume-test.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata: name: gvolume-test annotations: volume.beta.kubernetes.io/storage-class: gluster-heketispec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ~]# kubectl apply -f gluster-pvc-gvolume-test.yaml persistentvolumeclaim/gvolume-test created~]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEgvolume-test Bound pvc-9319d051-d1e7-11e8-ab5e-0050568faffd 1Gi RWO gluster-heketi 27s~]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-9319d051-d1e7-11e8-ab5e-0050568faffd 1Gi RWO Delete Bound default/gvolume-test gluster-heketi 20s 使用Pvc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#Pod引用pv[root@master01 pod]# cat gvolume.yaml apiVersion: v1kind: Podmetadata: name: gvolume-test namespace: default labels: app: gvolumespec: containers: - name: gvolume-test image: tomcat:8.5.34-alpine volumeMounts: - mountPath: /usr/local/tomcat/webapps/ROOT name: gvolume-test volumes: - name: gvolume-test persistentVolumeClaim: claimName: gvolume-test [root@master01 pod]# kubectl apply -f gvolume.yaml pod/gvolume-test created#登陆Pod查看mount卷[root@master01 pod]# kubectl exec -ti gvolume-test -- /bin/sh/usr/local/tomcat # mount | grep glusterfs172.20.40.191:vol_930bd0a752f309da01c3e55e785e2d50 on /usr/local/tomcat/webapps/ROOT type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)#在卷中写入内容/usr/local/tomcat # cd /usr/local/tomcat/webapps/ROOT/usr/local/tomcat/webapps/ROOT # touch index.html/usr/local/tomcat/webapps/ROOT # echo &quot;hello glusterfs&quot; &gt;&gt; index.html #查看PV对应的Volume ID[root@master01 pod]# kubectl describe pv pvc-9319d051-d1e7-11e8-ab5e-0050568faffdName: pvc-9319d051-d1e7-11e8-ab5e-0050568faffdLabels: &lt;none&gt;Annotations: Description: Gluster-Internal: Dynamically provisioned PV gluster.kubernetes.io/heketi-volume-id: 930bd0a752f309da01c3e55e785e2d50 #hekeit的vg名称,即volume ID gluster.org/type: file kubernetes.io/createdby: heketi-dynamic-provisioner pv.beta.kubernetes.io/gid: 2000 pv.kubernetes.io/bound-by-controller: yes pv.kubernetes.io/provisioned-by: kubernetes.io/glusterfsFinalizers: [kubernetes.io/pv-protection]StorageClass: gluster-heketiStatus: BoundClaim: default/gvolume-testReclaim Policy: DeleteAccess Modes: RWOCapacity: 1GiNode Affinity: &lt;none&gt;Message: Source: Type: Glusterfs (a Glusterfs mount on the host that shares a pod&apos;s lifetime) EndpointsName: glusterfs-dynamic-gvolume-test Path: vol_930bd0a752f309da01c3e55e785e2d50 ReadOnly: falseEvents: &lt;none&gt;#通过heketi查看volume分布的服务器位置#通过VolumeID找到所对应的Node节点，即storage01.isesol.local节点[root@master01 ~]# kubectl exec -ti heketi-754dfc7cdf-5gdcq -- /bin/shsh-4.4# heketi-cli topology infoName: vol_930bd0a752f309da01c3e55e785e2d50 Size: 1 Id: 930bd0a752f309da01c3e55e785e2d50 Cluster Id: fbb265f09857f98ad021368ba891562b Mount: 172.20.40.191:vol_930bd0a752f309da01c3e55e785e2d50 Mount Options: backup-volfile-servers=172.20.40.196,172.20.40.192,172.20.40.195,172.20.40.193,172.20.40.194 Durability Type: replicate Replica: 3 Snapshot: Enabled Snapshot Factor: 1.00 Bricks: Id: 1adaa7d5c825c7f15c2d30fc9c723bb8 Path: /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/brick Size (GiB): 1 Node: 185e10d74fb310c5d529112aa7b83eec Device: f49b75bdfe1165d8340421459d4413f1 Id: 759b1139ce5ab60d2fc159514de42e33 Path: /var/lib/heketi/mounts/vg_c44590f9656ad0f015df01f2b3d206a5/brick_759b1139ce5ab60d2fc159514de42e33/brick Size (GiB): 1 Node: f910638499d3a19403c45389c0605abf Device: c44590f9656ad0f015df01f2b3d206a5 Id: dc6a7be99d9307e6494e5bbd96512fc1 Path: /var/lib/heketi/mounts/vg_bddab02435633f4ca495431032b552c4/brick_dc6a7be99d9307e6494e5bbd96512fc1/brick Size (GiB): 1 Node: c78f3d4047816e23282a6ec8d743bc58 Device: bddab02435633f4ca495431032b552c4 Nodes: Node Id: 185e10d74fb310c5d529112aa7b83eec State: online Cluster Id: fbb265f09857f98ad021368ba891562b Zone: 1 Management Hostnames: storage01.isesol.local Storage Hostnames: 172.20.40.191 Devices: Id:f49b75bdfe1165d8340421459d4413f1 Name:/dev/sdb State:online Size (GiB):499 Used (GiB):1 Free (GiB):498 Bricks: Id:1adaa7d5c825c7f15c2d30fc9c723bb8 Size (GiB):1 Path: /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/brick #登陆storage01.isesol.local glusterfs节点查看[root@master01 ~]# kubectl exec -ti glusterfs-wn7jw -- /bin/shsh-4.2# cd /var/lib/heketi/mounts/vg_f49b75bdfe1165d8340421459d4413f1/brick_1adaa7d5c825c7f15c2d30fc9c723bb8/bricksh-4.2# lsindex.htmlsh-4.2# cat index.html hello glusterfs Heketi维护查看节点 1234567sh-4.4# heketi-cli node listId:185e10d74fb310c5d529112aa7b83eec Cluster:fbb265f09857f98ad021368ba891562bId:8f8f3413d57993a9014c834180136573 Cluster:fbb265f09857f98ad021368ba891562bId:c17dc162dc6ca73a025cafdef2f49f66 Cluster:fbb265f09857f98ad021368ba891562bId:c78f3d4047816e23282a6ec8d743bc58 Cluster:fbb265f09857f98ad021368ba891562bId:cb3e295cde4ead0077d22ba65215b2cb Cluster:fbb265f09857f98ad021368ba891562bId:f910638499d3a19403c45389c0605abf Cluster:fbb265f09857f98ad021368ba891562b 查看volume 123sh-4.4# heketi-cli volume list Id:7eef8baa9a616e8e9db78fc71f862498 Cluster:fbb265f09857f98ad021368ba891562b Name:heketidbstorageId:930bd0a752f309da01c3e55e785e2d50 Cluster:fbb265f09857f98ad021368ba891562b Name:vol_930bd0a752f309da01c3e55e785e2d50 查看拓扑信息 1sh-4.4# heketi-cli topology info 查看集群信息123sh-4.4# heketi-cli cluster listClusters:Id:fbb265f09857f98ad021368ba891562b [file][block] 扩展集群节点 如果集群节点的复本数是3，需要同时添加3个节点，如果复本数是2，即需要同时添加两个节点 1234567891011121314151617181920212223$ heketi-cli node add \ --zone=3 \ --cluster=3e21671bc4f290fca6bce464ae7bb6e7 \ --management-host-name=node1-manage.gluster.lab.com \ --storage-host-name=172.18.10.53Node information:Id: e0017385b683c10e4166492e78832d09State: onlineCluster Id: 3e21671bc4f290fca6bce464ae7bb6e7Zone: 3Management Hostname node1-manage.gluster.lab.comStorage Hostname 172.18.10.53$ heketi-cli device add \ --name=/dev/sdb \ --node=e0017385b683c10e4166492e78832d09Device added successfully$ heketi-cli device add \ --name=/dev/sdc \ --node=e0017385b683c10e4166492e78832d09Device added successfully]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
  </entry>
</search>
